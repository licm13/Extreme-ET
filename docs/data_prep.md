# æ•°æ®å‡†å¤‡æŒ‡å— (Data Preparation Guide)

## ç›®å½• (Table of Contents)

1. [æ•°æ®æ ¼å¼è¦æ±‚](#1-æ•°æ®æ ¼å¼è¦æ±‚)
2. [NetCDF æ–‡ä»¶å¤„ç†](#2-netcdf-æ–‡ä»¶å¤„ç†)
3. [æ•°æ®æ¸…æ´—ä¸è´¨é‡æ§åˆ¶](#3-æ•°æ®æ¸…æ´—ä¸è´¨é‡æ§åˆ¶)
4. [ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥](#4-ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥)
5. [æ—¶é—´åºåˆ—å¯¹é½ä¸é‡é‡‡æ ·](#5-æ—¶é—´åºåˆ—å¯¹é½ä¸é‡é‡‡æ ·)
6. [ç©ºé—´æ•°æ®å¤„ç†](#6-ç©ºé—´æ•°æ®å¤„ç†)
7. [å¸¸è§æ•°æ®æºæ¥å…¥](#7-å¸¸è§æ•°æ®æºæ¥å…¥)
8. [å®Œæ•´å·¥ä½œæµç¤ºä¾‹](#8-å®Œæ•´å·¥ä½œæµç¤ºä¾‹)

---

## 1. æ•°æ®æ ¼å¼è¦æ±‚

### 1.1 æ ¸å¿ƒè¾“å…¥æ•°æ®

Extreme-ET å·¥å…·åŒ…éœ€è¦ä»¥ä¸‹æ°”è±¡å˜é‡æ¥è®¡ç®— ETâ‚€ï¼š

| å˜é‡ | ç¬¦å· | å•ä½ | å¿…éœ€æ€§ | å¤‡æ³¨ |
|------|------|------|--------|------|
| å¹³å‡æ°”æ¸© | T_mean | Â°C | âœ… å¿…éœ€ | å¯ç”± T_max å’Œ T_min è®¡ç®— |
| æœ€é«˜æ°”æ¸© | T_max | Â°C | âœ… å¿…éœ€ | ç”¨äºè®¡ç®—é¥±å’Œæ°´æ±½å‹ |
| æœ€ä½æ°”æ¸© | T_min | Â°C | âœ… å¿…éœ€ | ç”¨äºè®¡ç®—é¥±å’Œæ°´æ±½å‹ |
| å¤ªé˜³è¾å°„ | Rs | MJ mâ»Â² dayâ»Â¹ | âœ… å¿…éœ€ | çŸ­æ³¢å…¥å°„è¾å°„ |
| é£é€Ÿ | uâ‚‚ | m sâ»Â¹ | âœ… å¿…éœ€ | 2ç±³é«˜åº¦é£é€Ÿ |
| å®é™…æ°´æ±½å‹ | ea | kPa | âœ… å¿…éœ€ | æˆ–é€šè¿‡ç›¸å¯¹æ¹¿åº¦è®¡ç®— |
| æµ·æ‹”é«˜åº¦ | z | m | ğŸ”¶ æ¨è | å½±å“æ°”å‹ä¿®æ­£ |
| çº¬åº¦ | latitude | Â°N | ğŸ”¶ æ¨è | ç”¨äºè¾å°„è®¡ç®— |
| æ—¥åºæ•° | DOY | 1-365 | ğŸ”¶ æ¨è | ç”¨äºç²¾ç¡®è¾å°„è®¡ç®— |

### 1.2 æ•°æ®ç»“æ„

**ä¸€ç»´æ—¶é—´åºåˆ—ï¼ˆç«™ç‚¹æ•°æ®ï¼‰ï¼š**

```python
# NumPy æ•°ç»„æ ¼å¼
data = {
    'time': np.array(['2000-01-01', ..., '2020-12-31'], dtype='datetime64'),
    'T_mean': np.array([...]),  # å½¢çŠ¶: (n_days,)
    'T_max': np.array([...]),
    'T_min': np.array([...]),
    'Rs': np.array([...]),
    'u2': np.array([...]),
    'ea': np.array([...])
}

# Pandas DataFrame æ ¼å¼ï¼ˆæ¨èï¼‰
import pandas as pd
df = pd.DataFrame({
    'T_mean': [...],
    'T_max': [...],
    'T_min': [...],
    'Rs': [...],
    'u2': [...],
    'ea': [...]
}, index=pd.date_range('2000-01-01', '2020-12-31', freq='D'))
```

**å¤šç»´ç½‘æ ¼æ•°æ®ï¼ˆNetCDFï¼‰ï¼š**

```python
# xarray.Dataset æ ¼å¼
import xarray as xr
ds = xr.Dataset({
    'T_mean': (['time', 'lat', 'lon'], data_3d),
    'Rs': (['time', 'lat', 'lon'], data_3d),
    # ...
}, coords={
    'time': pd.date_range('2000-01-01', '2020-12-31', freq='D'),
    'lat': np.arange(25, 50, 0.25),
    'lon': np.arange(-125, -66, 0.25)
})
```

---

## 2. NetCDF æ–‡ä»¶å¤„ç†

### 2.1 åŸºç¡€è¯»å–æ“ä½œ

**ä½¿ç”¨ xarrayï¼ˆæ¨èï¼‰ï¼š**

```python
import xarray as xr

# è¯»å–å•ä¸ªæ–‡ä»¶
ds = xr.open_dataset('path/to/data.nc')

# æŸ¥çœ‹å˜é‡åˆ—è¡¨
print(ds.data_vars)

# è¯»å–å¤šä¸ªæ–‡ä»¶ï¼ˆæŒ‰æ—¶é—´åˆå¹¶ï¼‰
ds = xr.open_mfdataset('data/*.nc', combine='by_coords')

# é€‰æ‹©ç‰¹å®šå˜é‡
da = ds['temperature']  # DataArray

# æŸ¥çœ‹å±æ€§
print(da.attrs)  # å…ƒæ•°æ®
print(da.coords)  # åæ ‡ä¿¡æ¯
```

### 2.2 å·¥å…·åŒ…æä¾›çš„ I/O å‡½æ•°

**æ–‡ä»¶**: `src/io_utils.py`

```python
from src.io_utils import read_netcdf_variable, sample_series_at_point

# è¯»å– NetCDF å˜é‡
da, lats, lons, times = read_netcdf_variable(
    filepath='era5_land_et.nc',
    varname='evaporation',
    lat_slice=(30, 45),  # çº¬åº¦èŒƒå›´
    lon_slice=(-120, -100),  # ç»åº¦èŒƒå›´
    time_slice=('2000-01-01', '2020-12-31')  # æ—¶é—´èŒƒå›´
)

print(f"æ•°æ®å½¢çŠ¶: {da.shape}")  # (æ—¶é—´, çº¬åº¦, ç»åº¦)
print(f"æ—¶é—´èŒƒå›´: {times[0]} åˆ° {times[-1]}")
```

### 2.3 æå–ç‰¹å®šä½ç½®çš„æ—¶é—´åºåˆ—

**æœ€è¿‘é‚»æ’å€¼ï¼ˆNearest Neighborï¼‰ï¼š**

```python
# æå–æ´›æ‰çŸ¶çš„æ—¶é—´åºåˆ—
lat_target, lon_target = 34.05, -118.24

series_nn = sample_series_at_point(
    da,
    lat_target,
    lon_target,
    method='nearest'
)

print(f"æå–çš„åºåˆ—é•¿åº¦: {len(series_nn)}")
print(f"å‰5ä¸ªå€¼: {series_nn[:5].values}")
```

**åŒçº¿æ€§æ’å€¼ï¼ˆBilinearï¼‰ï¼š**

```python
# æ›´å¹³æ»‘çš„æ’å€¼ç»“æœ
series_bl = sample_series_at_point(
    da,
    lat_target,
    lon_target,
    method='bilinear'
)

# å¯¹æ¯”ä¸¤ç§æ–¹æ³•
import matplotlib.pyplot as plt
plt.plot(series_nn, label='Nearest', alpha=0.7)
plt.plot(series_bl, label='Bilinear', alpha=0.7)
plt.legend()
plt.show()
```

### 2.4 æ‰¹é‡æå–å¤šä¸ªç«™ç‚¹

```python
def extract_multiple_stations(filepath, varname, stations):
    """
    æ‰¹é‡æå–å¤šä¸ªç«™ç‚¹çš„æ—¶é—´åºåˆ—

    Parameters
    ----------
    filepath : str
        NetCDF æ–‡ä»¶è·¯å¾„
    varname : str
        å˜é‡å
    stations : dict
        ç«™ç‚¹å­—å…¸ï¼Œæ ¼å¼: {'ç«™ç‚¹å': (çº¬åº¦, ç»åº¦)}

    Returns
    -------
    station_data : dict
        æ¯ä¸ªç«™ç‚¹çš„æ—¶é—´åºåˆ—
    """
    da, lats, lons, times = read_netcdf_variable(filepath, varname)

    station_data = {}
    for station_name, (lat, lon) in stations.items():
        series = sample_series_at_point(da, lat, lon, method='bilinear')
        station_data[station_name] = pd.Series(
            series.values,
            index=times,
            name=station_name
        )

    return pd.DataFrame(station_data)

# ä½¿ç”¨ç¤ºä¾‹
stations = {
    'Los Angeles': (34.05, -118.24),
    'Chicago': (41.88, -87.63),
    'New York': (40.71, -74.01)
}

station_df = extract_multiple_stations('data.nc', 'temperature', stations)
print(station_df.head())
```

---

## 3. æ•°æ®æ¸…æ´—ä¸è´¨é‡æ§åˆ¶

### 3.1 æ£€æµ‹å¼‚å¸¸å€¼

**ç‰©ç†èŒƒå›´æ£€æŸ¥ï¼š**

```python
def check_physical_bounds(data, varname):
    """
    æ£€æŸ¥å˜é‡æ˜¯å¦åœ¨ç‰©ç†åˆç†èŒƒå›´å†…

    Parameters
    ----------
    data : np.ndarray
        å˜é‡æ•°æ®
    varname : str
        å˜é‡åç§°

    Returns
    -------
    valid_mask : np.ndarray (bool)
        æœ‰æ•ˆæ•°æ®æ©ç 
    """
    bounds = {
        'T_mean': (-60, 60),     # Â°C
        'T_max': (-50, 70),
        'T_min': (-70, 50),
        'Rs': (0, 40),           # MJ/mÂ²/day
        'u2': (0, 50),           # m/s
        'ea': (0, 7),            # kPa
        'ET0': (0, 20)           # mm/day
    }

    if varname not in bounds:
        return np.ones(len(data), dtype=bool)

    lower, upper = bounds[varname]
    valid_mask = (data >= lower) & (data <= upper)

    n_invalid = np.sum(~valid_mask)
    if n_invalid > 0:
        print(f"è­¦å‘Š: {varname} æœ‰ {n_invalid} ä¸ªå€¼è¶…å‡ºèŒƒå›´ [{lower}, {upper}]")

    return valid_mask

# ä½¿ç”¨ç¤ºä¾‹
T_mean = np.array([15, 20, -999, 25, 30])  # -999 æ˜¯ç¼ºå¤±å€¼æ ‡è¯†
valid = check_physical_bounds(T_mean, 'T_mean')
T_mean_cleaned = np.where(valid, T_mean, np.nan)
```

**ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹ï¼ˆ3Ïƒ æ³•åˆ™ï¼‰ï¼š**

```python
def detect_outliers_zscore(data, threshold=3.0):
    """
    ä½¿ç”¨ Z-score æ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼

    Parameters
    ----------
    data : np.ndarray
        æ—¶é—´åºåˆ—æ•°æ®
    threshold : float, default=3.0
        Z-score é˜ˆå€¼ï¼ˆé€šå¸¸ 3 è¡¨ç¤º 99.7% ç½®ä¿¡ï¼‰

    Returns
    -------
    outlier_mask : np.ndarray (bool)
        å¼‚å¸¸å€¼æ©ç ï¼ˆTrue = å¼‚å¸¸ï¼‰
    """
    # ç§»é™¤ NaN åè®¡ç®—ç»Ÿè®¡é‡
    valid_data = data[~np.isnan(data)]
    mean = np.mean(valid_data)
    std = np.std(valid_data)

    z_scores = np.abs((data - mean) / std)
    outlier_mask = z_scores > threshold

    return outlier_mask

# ä½¿ç”¨ç¤ºä¾‹
outliers = detect_outliers_zscore(T_mean_cleaned)
print(f"æ£€æµ‹åˆ° {np.sum(outliers)} ä¸ªå¼‚å¸¸å€¼")
```

**åŸºäº IQR çš„ç¨³å¥æ£€æµ‹ï¼š**

```python
def detect_outliers_iqr(data, factor=1.5):
    """
    ä½¿ç”¨å››åˆ†ä½è· (IQR) æ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼

    Parameters
    ----------
    data : np.ndarray
        æ—¶é—´åºåˆ—æ•°æ®
    factor : float, default=1.5
        IQR å€æ•°ï¼ˆ1.5 æ˜¯æ ‡å‡†ï¼Œ3.0 æ˜¯æç«¯ï¼‰

    Returns
    -------
    outlier_mask : np.ndarray (bool)
        å¼‚å¸¸å€¼æ©ç 
    """
    valid_data = data[~np.isnan(data)]
    Q1 = np.percentile(valid_data, 25)
    Q3 = np.percentile(valid_data, 75)
    IQR = Q3 - Q1

    lower_bound = Q1 - factor * IQR
    upper_bound = Q3 + factor * IQR

    outlier_mask = (data < lower_bound) | (data > upper_bound)
    return outlier_mask
```

### 3.2 æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥

**æ¸©åº¦é€»è¾‘æ£€æŸ¥ï¼š**

```python
def check_temperature_consistency(T_mean, T_max, T_min):
    """
    æ£€æŸ¥æ¸©åº¦æ•°æ®çš„é€»è¾‘ä¸€è‡´æ€§

    è¦æ±‚: T_min <= T_mean <= T_max
    """
    inconsistent = (T_min > T_mean) | (T_mean > T_max) | (T_min > T_max)

    n_errors = np.sum(inconsistent)
    if n_errors > 0:
        print(f"è­¦å‘Š: å‘ç° {n_errors} ä¸ªæ¸©åº¦ä¸ä¸€è‡´çš„è®°å½•")

        # å°è¯•è‡ªåŠ¨ä¿®å¤ï¼ˆä½¿ç”¨å¹³å‡å€¼ï¼‰
        T_mean_fixed = np.where(
            inconsistent,
            (T_max + T_min) / 2,
            T_mean
        )
        return T_mean_fixed, inconsistent

    return T_mean, inconsistent

# ä½¿ç”¨ç¤ºä¾‹
T_mean_fixed, errors = check_temperature_consistency(T_mean, T_max, T_min)
```

---

## 4. ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥

### 4.1 ç¼ºå¤±å€¼è¯Šæ–­

```python
def diagnose_missing_data(df):
    """
    è¯Šæ–­æ•°æ®æ¡†ä¸­çš„ç¼ºå¤±æƒ…å†µ

    Parameters
    ----------
    df : pd.DataFrame
        åŒ…å«æ°”è±¡å˜é‡çš„æ•°æ®æ¡†

    Returns
    -------
    report : pd.DataFrame
        ç¼ºå¤±å€¼è¯Šæ–­æŠ¥å‘Š
    """
    report = pd.DataFrame({
        'n_missing': df.isnull().sum(),
        'pct_missing': df.isnull().sum() / len(df) * 100,
        'n_consecutive_max': [
            df[col].isnull().astype(int).groupby(
                df[col].notnull().astype(int).cumsum()
            ).sum().max()
            for col in df.columns
        ]
    })

    print("=== ç¼ºå¤±å€¼è¯Šæ–­æŠ¥å‘Š ===")
    print(report)

    # å¯è§†åŒ–ç¼ºå¤±æ¨¡å¼
    import matplotlib.pyplot as plt
    import seaborn as sns

    plt.figure(figsize=(12, 4))
    sns.heatmap(df.isnull().T, cbar=False, cmap='viridis', yticklabels=df.columns)
    plt.title('Missing Data Pattern')
    plt.xlabel('Time Index')
    plt.show()

    return report
```

### 4.2 çº¿æ€§æ’å€¼å¡«è¡¥

**ç®€å•çº¿æ€§æ’å€¼ï¼š**

```python
def fill_missing_linear(data, max_gap=7):
    """
    ä½¿ç”¨çº¿æ€§æ’å€¼å¡«è¡¥ç¼ºå¤±å€¼

    Parameters
    ----------
    data : np.ndarray or pd.Series
        æ—¶é—´åºåˆ—æ•°æ®
    max_gap : int, default=7
        å…è®¸æ’å€¼çš„æœ€å¤§è¿ç»­ç¼ºå¤±å¤©æ•°

    Returns
    -------
    data_filled : np.ndarray or pd.Series
        å¡«è¡¥åçš„æ•°æ®
    """
    if isinstance(data, pd.Series):
        # Pandas Series æ–¹æ³•
        data_filled = data.interpolate(
            method='linear',
            limit=max_gap,
            limit_direction='both'
        )
    else:
        # NumPy æ–¹æ³•
        from scipy.interpolate import interp1d

        valid_indices = ~np.isnan(data)
        if np.sum(valid_indices) < 2:
            return data  # æ— æ³•æ’å€¼

        x = np.arange(len(data))[valid_indices]
        y = data[valid_indices]

        f = interp1d(x, y, kind='linear', fill_value='extrapolate')
        data_filled = f(np.arange(len(data)))

        # åªå¡«è¡¥å°äº max_gap çš„ç¼ºå¤±
        gap_sizes = _calculate_gap_sizes(data)
        data_filled = np.where(gap_sizes <= max_gap, data_filled, np.nan)

    return data_filled

def _calculate_gap_sizes(data):
    """è®¡ç®—æ¯ä¸ªç¼ºå¤±å€¼æ‰€åœ¨ç¼ºå¤±æ®µçš„é•¿åº¦"""
    is_nan = np.isnan(data)
    gap_id = (~is_nan).cumsum()  # ç»™æ¯ä¸ªç¼ºå¤±æ®µåˆ†é…ID
    gap_sizes = np.zeros(len(data))

    for gid in np.unique(gap_id[is_nan]):
        gap_mask = (gap_id == gid) & is_nan
        gap_sizes[gap_mask] = np.sum(gap_mask)

    return gap_sizes
```

**ç¤ºä¾‹ï¼ˆæ¥è‡ª `examples/example_zhao_2025.py`ï¼‰ï¼š**

```python
def _fill_nan_linear(arr, max_gap=7):
    """
    çº¿æ€§æ’å€¼å¡«è¡¥NaNï¼Œä½†è·³è¿‡è¿‡é•¿çš„ç¼ºå¤±æ®µ

    è¿™æ˜¯ Zhao et al. (2025) è®ºæ–‡ä¸­ä½¿ç”¨çš„æ–¹æ³•
    """
    arr = np.asarray(arr, dtype=float)
    idx = np.arange(len(arr))
    valid = ~np.isnan(arr)

    if np.sum(valid) < 2:
        return arr

    # ä½¿ç”¨ scipy è¿›è¡Œæ’å€¼
    from scipy.interpolate import interp1d
    f = interp1d(idx[valid], arr[valid], kind='linear',
                 bounds_error=False, fill_value=np.nan)
    arr_interp = f(idx)

    # è¯†åˆ«ç¼ºå¤±æ®µ
    nan_mask = np.isnan(arr)
    nan_segments = np.split(np.arange(len(arr)),
                           np.where(np.diff(nan_mask.astype(int)) != 0)[0] + 1)

    # åªå¡«è¡¥çŸ­ç¼ºå¤±æ®µ
    arr_filled = arr.copy()
    for segment in nan_segments:
        if len(segment) > 0 and nan_mask[segment[0]]:
            if len(segment) <= max_gap:
                arr_filled[segment] = arr_interp[segment]

    return arr_filled
```

### 4.3 æ°”å€™å­¦å¡«è¡¥

**ä½¿ç”¨å¤šå¹´å¹³å‡å€¼ï¼š**

```python
def fill_missing_climatology(data, dates):
    """
    ä½¿ç”¨æ°”å€™å­¦å¹³å‡å€¼å¡«è¡¥ç¼ºå¤±å€¼

    Parameters
    ----------
    data : pd.Series
        æ—¶é—´åºåˆ—ï¼ˆç´¢å¼•ä¸ºæ—¥æœŸï¼‰
    dates : pd.DatetimeIndex
        å¯¹åº”çš„æ—¥æœŸç´¢å¼•

    Returns
    -------
    data_filled : pd.Series
        å¡«è¡¥åçš„æ•°æ®
    """
    # è®¡ç®—æ¯ä¸ªæ—¥å†æ—¥ï¼ˆDOYï¼‰çš„å¤šå¹´å¹³å‡
    doy = dates.dayofyear
    climatology = data.groupby(doy).mean()

    # ç”¨æ°”å€™å­¦å€¼å¡«è¡¥ç¼ºå¤±
    data_filled = data.copy()
    missing_mask = data.isnull()
    data_filled[missing_mask] = climatology[doy[missing_mask]].values

    return data_filled

# ä½¿ç”¨ç¤ºä¾‹
df = pd.DataFrame({
    'ET0': [3.5, np.nan, 4.2, np.nan, 5.1],
}, index=pd.date_range('2020-01-01', periods=5))

df['ET0_filled'] = fill_missing_climatology(df['ET0'], df.index)
```

### 4.4 å¤šå˜é‡æ’è¡¥ï¼ˆé«˜çº§ï¼‰

**ä½¿ç”¨ç›¸å…³å˜é‡é¢„æµ‹ï¼š**

```python
from sklearn.linear_model import LinearRegression

def fill_missing_multivariate(df, target_var, predictor_vars):
    """
    ä½¿ç”¨å¤šå…ƒçº¿æ€§å›å½’å¡«è¡¥ç¼ºå¤±å€¼

    ä¾‹å¦‚ï¼šç”¨æ¸©åº¦å’Œè¾å°„é¢„æµ‹ç¼ºå¤±çš„é£é€Ÿ

    Parameters
    ----------
    df : pd.DataFrame
        åŒ…å«æ‰€æœ‰å˜é‡çš„æ•°æ®æ¡†
    target_var : str
        éœ€è¦å¡«è¡¥çš„ç›®æ ‡å˜é‡
    predictor_vars : list of str
        ç”¨äºé¢„æµ‹çš„å˜é‡åˆ—è¡¨

    Returns
    -------
    df_filled : pd.DataFrame
        å¡«è¡¥åçš„æ•°æ®æ¡†
    """
    # åˆ†ç¦»è®­ç»ƒé›†ï¼ˆå®Œæ•´æ•°æ®ï¼‰å’Œå¾…å¡«è¡¥é›†
    complete_mask = df[predictor_vars + [target_var]].notnull().all(axis=1)
    missing_mask = df[target_var].isnull() & df[predictor_vars].notnull().all(axis=1)

    if np.sum(missing_mask) == 0:
        return df

    # è®­ç»ƒæ¨¡å‹
    X_train = df.loc[complete_mask, predictor_vars]
    y_train = df.loc[complete_mask, target_var]

    model = LinearRegression()
    model.fit(X_train, y_train)

    # é¢„æµ‹ç¼ºå¤±å€¼
    X_missing = df.loc[missing_mask, predictor_vars]
    y_pred = model.predict(X_missing)

    # å¡«è¡¥
    df_filled = df.copy()
    df_filled.loc[missing_mask, target_var] = y_pred

    print(f"ä½¿ç”¨ {predictor_vars} å¡«è¡¥äº† {np.sum(missing_mask)} ä¸ª {target_var} çš„ç¼ºå¤±å€¼")
    print(f"æ¨¡å‹ RÂ² = {model.score(X_train, y_train):.3f}")

    return df_filled

# ä½¿ç”¨ç¤ºä¾‹
df_filled = fill_missing_multivariate(
    df,
    target_var='u2',  # å¡«è¡¥é£é€Ÿ
    predictor_vars=['T_mean', 'Rs']  # ä½¿ç”¨æ¸©åº¦å’Œè¾å°„
)
```

---

## 5. æ—¶é—´åºåˆ—å¯¹é½ä¸é‡é‡‡æ ·

### 5.1 æ—¶é—´å¯¹é½

**å¤„ç†ä¸åŒæ—¶åŒºï¼š**

```python
import pandas as pd

def align_to_utc(df, source_timezone='US/Pacific'):
    """
    å°†æœ¬åœ°æ—¶é—´è½¬æ¢ä¸º UTC

    Parameters
    ----------
    df : pd.DataFrame
        ç´¢å¼•ä¸ºæ—¥æœŸæ—¶é—´çš„æ•°æ®æ¡†
    source_timezone : str
        æºæ—¶åŒºï¼ˆå¦‚ 'US/Pacific', 'Europe/London'ï¼‰

    Returns
    -------
    df_utc : pd.DataFrame
        UTC æ—¶é—´çš„æ•°æ®æ¡†
    """
    df_utc = df.copy()
    df_utc.index = df_utc.index.tz_localize(source_timezone).tz_convert('UTC')
    return df_utc
```

**å¯¹é½åˆ°æ—¥ç•Œçº¿ï¼ˆUTC 0:00ï¼‰ï¼š**

```python
def align_to_daily(df, method='mean'):
    """
    å°†å­æ—¥å°ºåº¦æ•°æ®èšåˆä¸ºæ—¥æ•°æ®

    Parameters
    ----------
    df : pd.DataFrame
        é«˜é¢‘æ•°æ®ï¼ˆå¦‚å°æ—¶å°ºåº¦ï¼‰
    method : str
        èšåˆæ–¹æ³•ï¼š'mean', 'sum', 'max', 'min'

    Returns
    -------
    df_daily : pd.DataFrame
        æ—¥å°ºåº¦æ•°æ®
    """
    if method == 'mean':
        df_daily = df.resample('D').mean()
    elif method == 'sum':
        df_daily = df.resample('D').sum()
    elif method == 'max':
        df_daily = df.resample('D').max()
    elif method == 'min':
        df_daily = df.resample('D').min()
    else:
        raise ValueError(f"Unknown method: {method}")

    return df_daily

# ä½¿ç”¨ç¤ºä¾‹
# å‡è®¾æœ‰å°æ—¶æ•°æ®
hourly_data = pd.DataFrame({
    'T': np.random.rand(24*365),
}, index=pd.date_range('2020-01-01', periods=24*365, freq='H'))

daily_data = align_to_daily(hourly_data, method='mean')
```

### 5.2 é‡é‡‡æ ·åˆ°ä¸åŒæ—¶é—´åˆ†è¾¨ç‡

**ä¸Šé‡‡æ ·ï¼ˆæ—¥ â†’ å°æ—¶ï¼‰ï¼š**

```python
def upsample_with_diurnal_cycle(daily_temp, method='sine'):
    """
    å°†æ—¥æ•°æ®ä¸Šé‡‡æ ·ä¸ºå°æ—¶æ•°æ®ï¼ˆè€ƒè™‘æ—¥å˜åŒ–ï¼‰

    Parameters
    ----------
    daily_temp : pd.Series
        æ—¥å¹³å‡æ¸©åº¦
    method : str
        æ—¥å˜åŒ–æ¨¡å‹ï¼š'sine', 'linear'

    Returns
    -------
    hourly_temp : pd.Series
        å°æ—¶æ¸©åº¦
    """
    # åˆ›å»ºå°æ—¶ç´¢å¼•
    hourly_index = pd.date_range(
        daily_temp.index[0],
        daily_temp.index[-1] + pd.Timedelta(days=1),
        freq='H',
        inclusive='left'
    )

    # çº¿æ€§æ’å€¼
    hourly_temp = daily_temp.reindex(hourly_index).interpolate(method='linear')

    if method == 'sine':
        # å åŠ æ—¥å˜åŒ–ï¼ˆç®€åŒ–æ¨¡å‹ï¼‰
        hour_of_day = hourly_index.hour
        diurnal_cycle = 5 * np.sin((hour_of_day - 6) * np.pi / 12)  # å³°å€¼åœ¨14:00
        hourly_temp += diurnal_cycle

    return hourly_temp
```

**ä¸‹é‡‡æ ·ï¼ˆå°æ—¶ â†’ æ—¥ï¼‰ï¼š**

```python
# è§ä¸Šé¢çš„ align_to_daily å‡½æ•°
```

### 5.3 é—°å¹´å¤„ç†

```python
def remove_leap_days(df):
    """
    ç§»é™¤é—°å¹´çš„2æœˆ29æ—¥

    Parameters
    ----------
    df : pd.DataFrame
        åŒ…å«é—°å¹´çš„æ•°æ®æ¡†

    Returns
    -------
    df_no_leap : pd.DataFrame
        ç§»é™¤2æœˆ29æ—¥åçš„æ•°æ®æ¡†
    """
    df_no_leap = df[~((df.index.month == 2) & (df.index.day == 29))]
    print(f"ç§»é™¤äº† {len(df) - len(df_no_leap)} ä¸ªé—°å¹´æ—¥æœŸ")
    return df_no_leap

def expand_to_366days(data_365):
    """
    å°†365å¤©æ•°æ®æ‰©å±•ä¸º366å¤©ï¼ˆå¤åˆ¶2æœˆ28æ—¥ï¼‰

    Parameters
    ----------
    data_365 : np.ndarray or list
        é•¿åº¦ä¸º365çš„æ•°æ®

    Returns
    -------
    data_366 : np.ndarray
        é•¿åº¦ä¸º366çš„æ•°æ®
    """
    data_365 = np.asarray(data_365)
    # åœ¨ç´¢å¼•58ï¼ˆ2æœˆ28æ—¥ï¼‰åæ’å…¥ä¸€ä¸ªé‡å¤å€¼
    data_366 = np.insert(data_365, 59, data_365[58])
    return data_366
```

---

## 6. ç©ºé—´æ•°æ®å¤„ç†

### 6.1 åæ ‡ç³»ç»Ÿè½¬æ¢

```python
def convert_longitude_convention(lon):
    """
    è½¬æ¢ç»åº¦è¡¨ç¤ºæ³•

    0-360Â° â†” -180-180Â°

    Parameters
    ----------
    lon : float or np.ndarray
        ç»åº¦å€¼

    Returns
    -------
    lon_converted : float or np.ndarray
        è½¬æ¢åçš„ç»åº¦
    """
    lon = np.asarray(lon)

    # 0-360 -> -180-180
    lon_converted = np.where(lon > 180, lon - 360, lon)

    # å¦‚æœéœ€è¦åå‘è½¬æ¢ï¼š
    # lon_converted = np.where(lon < 0, lon + 360, lon)

    return lon_converted
```

### 6.2 ç½‘æ ¼é‡æ’å€¼

**åŒçº¿æ€§æ’å€¼åˆ°æ–°ç½‘æ ¼ï¼š**

```python
from scipy.interpolate import RegularGridInterpolator

def regrid_data(data_old, lat_old, lon_old, lat_new, lon_new):
    """
    å°†æ•°æ®ä»æ—§ç½‘æ ¼æ’å€¼åˆ°æ–°ç½‘æ ¼

    Parameters
    ----------
    data_old : np.ndarray (n_lat_old, n_lon_old)
        æ—§ç½‘æ ¼æ•°æ®
    lat_old, lon_old : np.ndarray
        æ—§ç½‘æ ¼çš„åæ ‡
    lat_new, lon_new : np.ndarray
        æ–°ç½‘æ ¼çš„åæ ‡

    Returns
    -------
    data_new : np.ndarray (n_lat_new, n_lon_new)
        æ–°ç½‘æ ¼æ•°æ®
    """
    # åˆ›å»ºæ’å€¼å‡½æ•°
    interp_func = RegularGridInterpolator(
        (lat_old, lon_old),
        data_old,
        method='linear',
        bounds_error=False,
        fill_value=np.nan
    )

    # åˆ›å»ºæ–°ç½‘æ ¼çš„åæ ‡ç½‘æ ¼
    lon_new_grid, lat_new_grid = np.meshgrid(lon_new, lat_new)
    points_new = np.column_stack([
        lat_new_grid.ravel(),
        lon_new_grid.ravel()
    ])

    # æ’å€¼
    data_new = interp_func(points_new).reshape(len(lat_new), len(lon_new))

    return data_new
```

**ä½¿ç”¨ xarray é‡æ’å€¼ï¼ˆæ›´ç®€å•ï¼‰ï¼š**

```python
def regrid_xarray(ds, target_grid):
    """
    ä½¿ç”¨ xarray é‡æ’å€¼

    Parameters
    ----------
    ds : xr.Dataset
        æºæ•°æ®é›†
    target_grid : xr.Dataset
        ç›®æ ‡ç½‘æ ¼ï¼ˆæä¾› lat/lon åæ ‡ï¼‰

    Returns
    -------
    ds_regridded : xr.Dataset
        é‡æ’å€¼åçš„æ•°æ®é›†
    """
    ds_regridded = ds.interp(
        lat=target_grid.lat,
        lon=target_grid.lon,
        method='linear'
    )
    return ds_regridded
```

### 6.3 ç©ºé—´èšåˆ

**è®¡ç®—åŒºåŸŸå¹³å‡ï¼š**

```python
def calculate_regional_mean(da, lat_bounds, lon_bounds, weights='cosine'):
    """
    è®¡ç®—åŒºåŸŸå¹³å‡ï¼ˆè€ƒè™‘çº¬åº¦æƒé‡ï¼‰

    Parameters
    ----------
    da : xr.DataArray
        æ•°æ®æ•°ç»„ï¼ˆç»´åº¦: time, lat, lonï¼‰
    lat_bounds : tuple
        çº¬åº¦èŒƒå›´ (lat_min, lat_max)
    lon_bounds : tuple
        ç»åº¦èŒƒå›´ (lon_min, lon_max)
    weights : str
        æƒé‡æ–¹æ¡ˆï¼š'cosine'ï¼ˆä½™å¼¦çº¬åº¦æƒé‡ï¼‰æˆ– 'equal'ï¼ˆç­‰æƒï¼‰

    Returns
    -------
    regional_mean : xr.DataArray
        åŒºåŸŸå¹³å‡æ—¶é—´åºåˆ—
    """
    # é€‰æ‹©åŒºåŸŸ
    da_region = da.sel(
        lat=slice(*lat_bounds),
        lon=slice(*lon_bounds)
    )

    if weights == 'cosine':
        # è®¡ç®—çº¬åº¦æƒé‡ï¼ˆå› ä¸ºç½‘æ ¼å•å…ƒé¢ç§¯éšçº¬åº¦å˜åŒ–ï¼‰
        lat_weights = np.cos(np.deg2rad(da_region.lat))
        lat_weights = lat_weights / lat_weights.sum()

        # åŠ æƒå¹³å‡
        regional_mean = (da_region * lat_weights).sum(dim=['lat', 'lon'])
    else:
        # ç®€å•å¹³å‡
        regional_mean = da_region.mean(dim=['lat', 'lon'])

    return regional_mean

# ä½¿ç”¨ç¤ºä¾‹
# è®¡ç®—ç¾å›½å¤§å¹³åŸçš„å¹³å‡ ET0
great_plains_et = calculate_regional_mean(
    da,
    lat_bounds=(35, 45),
    lon_bounds=(-105, -95),
    weights='cosine'
)
```

---

## 7. å¸¸è§æ•°æ®æºæ¥å…¥

### 7.1 ERA5-Land

```python
def load_era5_land(filepath, variables, lat_range=None, lon_range=None):
    """
    åŠ è½½ ERA5-Land æ•°æ®

    ERA5-Land æ•°æ®ç‰¹ç‚¹ï¼š
    - åˆ†è¾¨ç‡ï¼š0.1Â° Ã— 0.1Â°
    - æ—¶é—´é¢‘ç‡ï¼šå°æ—¶
    - å˜é‡å‘½åï¼šå‚è€ƒ Copernicus CDS

    Parameters
    ----------
    filepath : str
        ERA5-Land NetCDF æ–‡ä»¶è·¯å¾„
    variables : list of str
        éœ€è¦çš„å˜é‡åˆ—è¡¨ï¼Œå¦‚ï¼š
        - 't2m': 2ç±³æ¸©åº¦ (K)
        - 'u10', 'v10': 10ç±³é£é€Ÿåˆ†é‡ (m/s)
        - 'ssrd': çŸ­æ³¢è¾å°„ (J/mÂ²)
        - 'd2m': 2ç±³éœ²ç‚¹æ¸©åº¦ (K)
    lat_range : tuple, optional
        çº¬åº¦èŒƒå›´ (lat_min, lat_max)
    lon_range : tuple, optional
        ç»åº¦èŒƒå›´ (lon_min, lon_max)

    Returns
    -------
    df : pd.DataFrame
        å¤„ç†åçš„æ•°æ®æ¡†ï¼ˆSIå•ä½ï¼‰
    """
    import xarray as xr

    # è¯»å–æ•°æ®
    ds = xr.open_dataset(filepath)

    # ç©ºé—´å­é›†
    if lat_range:
        ds = ds.sel(latitude=slice(*lat_range))
    if lon_range:
        ds = ds.sel(longitude=slice(*lon_range))

    # æå–å˜é‡å¹¶è½¬æ¢å•ä½
    data = {}

    for var in variables:
        if var == 't2m':
            # å¼€å°”æ–‡ -> æ‘„æ°åº¦
            data['T_mean'] = ds[var] - 273.15
        elif var == 'd2m':
            # éœ²ç‚¹æ¸©åº¦ -> å®é™…æ°´æ±½å‹
            data['ea'] = 0.6108 * np.exp(17.27 * (ds[var] - 273.15) / ((ds[var] - 273.15) + 237.3))
        elif var == 'ssrd':
            # ç´¯ç§¯è¾å°„ (J/mÂ²) -> æ—¥å¹³å‡ (MJ/mÂ²/day)
            # æ³¨æ„ï¼šERA5 çš„ ssrd æ˜¯ç´¯ç§¯å€¼ï¼Œéœ€è¦å·®åˆ†
            data['Rs'] = ds[var].diff('time') / 1e6  # J -> MJ
        elif var in ['u10', 'v10']:
            # 10ç±³é£é€Ÿ -> 2ç±³é£é€Ÿ
            if 'u10' in variables and 'v10' in variables:
                u10 = ds['u10']
                v10 = ds['v10']
                wind_10m = np.sqrt(u10**2 + v10**2)
                # ä½¿ç”¨å¯¹æ•°é£å»“çº¿è°ƒæ•´
                from src.penman_monteith import adjust_wind_speed
                data['u2'] = adjust_wind_speed(wind_10m, 10, 2)

    # è½¬æ¢ä¸º DataFrame
    df = xr.Dataset(data).to_dataframe()

    return df
```

### 7.2 gridMET

```python
def load_gridmet(base_path, year_start, year_end, lat, lon):
    """
    åŠ è½½ gridMET æ•°æ®ï¼ˆç¾å›½æœ¬åœŸé«˜åˆ†è¾¨ç‡æ•°æ®ï¼‰

    gridMET æ•°æ®ç‰¹ç‚¹ï¼š
    - åˆ†è¾¨ç‡ï¼šçº¦ 4 km
    - è¦†ç›–èŒƒå›´ï¼šCONUSï¼ˆç¾å›½æœ¬åœŸï¼‰
    - æ—¶é—´èŒƒå›´ï¼š1979-present
    - å·²è®¡ç®—å¥½ ET0ï¼

    Parameters
    ----------
    base_path : str
        gridMET æ•°æ®ç›®å½•
    year_start, year_end : int
        æ—¶é—´èŒƒå›´
    lat, lon : float
        ç›®æ ‡ä½ç½®

    Returns
    -------
    df : pd.DataFrame
        åŒ…å«æ‰€æœ‰æ°”è±¡å˜é‡å’Œ ET0
    """
    import xarray as xr
    from pathlib import Path

    variables = {
        'tmmx': 'T_max',
        'tmmn': 'T_min',
        'srad': 'Rs',
        'vs': 'u2',
        'etr': 'ET0'  # gridMET å·²ç»è®¡ç®—å¥½çš„ET0
    }

    data = {}

    for var_gridmet, var_name in variables.items():
        files = [
            f"{base_path}/{var_gridmet}_{year}.nc"
            for year in range(year_start, year_end + 1)
        ]

        # æ‰“å¼€å¤šæ–‡ä»¶
        ds = xr.open_mfdataset(files, combine='by_coords')

        # æå–æ—¶é—´åºåˆ—
        series = ds[var_gridmet].sel(
            lat=lat, lon=lon, method='nearest'
        ).values

        data[var_name] = series

    # åˆ›å»º DataFrame
    time_index = pd.date_range(
        f"{year_start}-01-01",
        f"{year_end}-12-31",
        freq='D'
    )
    df = pd.DataFrame(data, index=time_index)

    # å•ä½è½¬æ¢
    df['T_max'] = df['T_max'] - 273.15  # K -> Â°C
    df['T_min'] = df['T_min'] - 273.15
    df['Rs'] = df['Rs'] * 0.0864  # W/mÂ² -> MJ/mÂ²/day

    return df
```

### 7.3 ç«™ç‚¹è§‚æµ‹æ•°æ®

```python
def load_station_data(filepath, format='csv'):
    """
    åŠ è½½ç«™ç‚¹è§‚æµ‹æ•°æ®

    æ”¯æŒæ ¼å¼ï¼š
    - CSVï¼ˆé€šç”¨ï¼‰
    - GHCN-Dailyï¼ˆNOAA å…¨çƒå†å²æ°”å€™å­¦ç½‘ç»œï¼‰
    - CIMISï¼ˆåŠ å·çŒæº‰ç®¡ç†ä¿¡æ¯ç³»ç»Ÿï¼‰

    Parameters
    ----------
    filepath : str
        æ•°æ®æ–‡ä»¶è·¯å¾„
    format : str
        æ•°æ®æ ¼å¼

    Returns
    -------
    df : pd.DataFrame
        æ ‡å‡†åŒ–çš„æ•°æ®æ¡†
    """
    if format == 'csv':
        df = pd.read_csv(filepath, parse_dates=['date'], index_col='date')

        # å‡è®¾åˆ—åæ˜ å°„
        column_mapping = {
            'temp_max': 'T_max',
            'temp_min': 'T_min',
            'solar_rad': 'Rs',
            'wind_speed': 'u2',
            'rel_humidity': 'RH'
        }
        df = df.rename(columns=column_mapping)

        # å¦‚æœæœ‰ç›¸å¯¹æ¹¿åº¦ï¼Œè½¬æ¢ä¸ºæ°´æ±½å‹
        if 'RH' in df.columns and 'T_mean' in df.columns:
            es = 0.6108 * np.exp(17.27 * df['T_mean'] / (df['T_mean'] + 237.3))
            df['ea'] = es * df['RH'] / 100

    elif format == 'ghcn':
        # GHCN-Daily æ ¼å¼è¾ƒå¤æ‚ï¼Œéœ€è¦ç‰¹æ®Šè§£æ
        # å‚è€ƒï¼šhttps://www.ncei.noaa.gov/data/ghcn-daily/doc/
        pass  # å®ç°ç•¥

    return df
```

---

## 8. å®Œæ•´å·¥ä½œæµç¤ºä¾‹

### 8.1 ä»åŸå§‹ ERA5-Land åˆ°æç«¯æ£€æµ‹

```python
import numpy as np
import pandas as pd
import xarray as xr
from src.penman_monteith import calculate_et0
from src.extreme_detection import detect_extreme_events_clim
from src.contribution_analysis import calculate_contributions

# ========== æ­¥éª¤ 1: åŠ è½½æ•°æ® ==========
print("æ­£åœ¨åŠ è½½ ERA5-Land æ•°æ®...")
ds = xr.open_dataset('era5_land_2000_2020.nc')

# æå–æ´›æ‰çŸ¶çš„æ—¶é—´åºåˆ—
lat_target, lon_target = 34.05, -118.24
ds_site = ds.sel(lat=lat_target, lon=lon_target, method='nearest')

# ========== æ­¥éª¤ 2: å•ä½è½¬æ¢ ==========
print("è½¬æ¢å•ä½...")
T_max = ds_site['t2m_max'].values - 273.15  # K -> Â°C
T_min = ds_site['t2m_min'].values - 273.15
T_mean = (T_max + T_min) / 2
Rs = ds_site['ssrd'].values / 1e6  # J/mÂ² -> MJ/mÂ²
u10 = np.sqrt(ds_site['u10']**2 + ds_site['v10']**2).values

# é£é€Ÿé«˜åº¦è°ƒæ•´
from src.penman_monteith import adjust_wind_speed
u2 = adjust_wind_speed(u10, z_measurement=10, z_target=2)

# éœ²ç‚¹æ¸©åº¦ -> æ°´æ±½å‹
Td = ds_site['d2m'].values - 273.15
ea = 0.6108 * np.exp(17.27 * Td / (Td + 237.3))

# ========== æ­¥éª¤ 3: æ•°æ®æ¸…æ´— ==========
print("æ•°æ®æ¸…æ´—...")
# æ£€æŸ¥ç‰©ç†èŒƒå›´
valid = (
    (T_mean >= -50) & (T_mean <= 50) &
    (Rs >= 0) & (Rs <= 40) &
    (u2 >= 0) & (u2 <= 30) &
    (ea >= 0) & (ea <= 7)
)

print(f"ç§»é™¤äº† {np.sum(~valid)} ä¸ªæ— æ•ˆæ•°æ®ç‚¹")

T_mean[~valid] = np.nan
T_max[~valid] = np.nan
T_min[~valid] = np.nan
Rs[~valid] = np.nan
u2[~valid] = np.nan
ea[~valid] = np.nan

# çº¿æ€§æ’å€¼å¡«è¡¥å°ç¼ºå¤±
from scipy.interpolate import interp1d
for var in [T_mean, T_max, T_min, Rs, u2, ea]:
    valid_idx = ~np.isnan(var)
    if np.sum(valid_idx) > 10:
        f = interp1d(
            np.arange(len(var))[valid_idx],
            var[valid_idx],
            kind='linear',
            bounds_error=False,
            fill_value='extrapolate'
        )
        var[:] = f(np.arange(len(var)))

# ========== æ­¥éª¤ 4: è®¡ç®— ET0 ==========
print("è®¡ç®— ET0...")
et0 = calculate_et0(
    T_mean=T_mean,
    T_max=T_max,
    T_min=T_min,
    Rs=Rs,
    u2=u2,
    ea=ea,
    z=50.0,
    latitude=lat_target
)

# ========== æ­¥éª¤ 5: æç«¯äº‹ä»¶æ£€æµ‹ ==========
print("æ£€æµ‹æç«¯äº‹ä»¶...")
extreme_mask, thresholds = detect_extreme_events_clim(
    et0,
    severity=0.05,
    min_duration=3
)

print(f"æ£€æµ‹åˆ° {np.sum(extreme_mask)} ä¸ªæç«¯å¤©æ•°")

# ========== æ­¥éª¤ 6: é©±åŠ¨å› å­åˆ†æ ==========
print("åˆ†æé©±åŠ¨å› å­...")
contributions = calculate_contributions(
    T_mean=T_mean,
    T_max=T_max,
    T_min=T_min,
    Rs=Rs,
    u2=u2,
    ea=ea,
    extreme_mask=extreme_mask,
    z=50.0,
    latitude=lat_target
)

print("\né©±åŠ¨å› å­è´¡çŒ®ç‡ï¼š")
for factor, contrib in contributions.items():
    print(f"  {factor:15s}: {contrib:5.1f}%")

# ========== æ­¥éª¤ 7: å¯è§†åŒ– ==========
import matplotlib.pyplot as plt

fig, axes = plt.subplots(3, 1, figsize=(14, 10))

# å­å›¾1: ET0 æ—¶é—´åºåˆ—
axes[0].plot(et0, color='steelblue', linewidth=0.5, alpha=0.7)
axes[0].scatter(np.where(extreme_mask)[0], et0[extreme_mask],
               color='red', s=10, zorder=5, label='Extreme Events')
axes[0].set_ylabel('ETâ‚€ (mm/day)')
axes[0].set_title('Los Angeles: ETâ‚€ Time Series (2000-2020)')
axes[0].legend()
axes[0].grid(alpha=0.3)

# å­å›¾2: æ°”å€™å­¦é˜ˆå€¼
doy = np.arange(365)
axes[1].plot(doy, thresholds, color='orange', linewidth=2)
axes[1].set_xlabel('Day of Year')
axes[1].set_ylabel('Threshold (mm/day)')
axes[1].set_title('Climatological Threshold (95th Percentile)')
axes[1].grid(alpha=0.3)

# å­å›¾3: è´¡çŒ®ç‡é¥¼å›¾
axes[2].pie(
    contributions.values(),
    labels=contributions.keys(),
    autopct='%1.1f%%',
    startangle=90
)
axes[2].set_title('Driver Contributions to Extreme ET Events')

plt.tight_layout()
plt.savefig('extreme_et_analysis.png', dpi=300)
print("\nå›¾è¡¨å·²ä¿å­˜è‡³ extreme_et_analysis.png")
```

---

## 9. å¸¸è§é—®é¢˜ (FAQ)

### Q1: å¦‚ä½•å¤„ç†äºšæ—¥å°ºåº¦ï¼ˆå°æ—¶ï¼‰æ•°æ®ï¼Ÿ

**A**: é¦–å…ˆèšåˆåˆ°æ—¥å°ºåº¦ï¼š

```python
df_daily = df_hourly.resample('D').agg({
    'T': 'mean',
    'T_max': 'max',
    'T_min': 'min',
    'Rs': 'sum',  # è¾å°„éœ€è¦ç´¯ç§¯
    'u2': 'mean',
    'ea': 'mean'
})

# æ³¨æ„ï¼šè¾å°„å•ä½éœ€è¦ä» W/mÂ² è½¬æ¢ä¸º MJ/mÂ²/day
df_daily['Rs'] = df_daily['Rs'] * 3600 / 1e6  # WÂ·h/mÂ² -> MJ/mÂ²
```

### Q2: å¦‚ä½•ä»ç›¸å¯¹æ¹¿åº¦è®¡ç®—å®é™…æ°´æ±½å‹ï¼Ÿ

**A**: ä½¿ç”¨ä»¥ä¸‹å…¬å¼ï¼š

```python
def rh_to_ea(T, RH):
    """
    ä»ç›¸å¯¹æ¹¿åº¦è®¡ç®—å®é™…æ°´æ±½å‹

    Parameters
    ----------
    T : float or array-like
        æ°”æ¸© (Â°C)
    RH : float or array-like
        ç›¸å¯¹æ¹¿åº¦ (%)

    Returns
    -------
    ea : float or array-like
        å®é™…æ°´æ±½å‹ (kPa)
    """
    # é¥±å’Œæ°´æ±½å‹ï¼ˆTetens å…¬å¼ï¼‰
    es = 0.6108 * np.exp(17.27 * T / (T + 237.3))

    # å®é™…æ°´æ±½å‹
    ea = es * RH / 100

    return ea
```

### Q3: æ•°æ®é‡å¤ªå¤§ï¼Œå†…å­˜ä¸å¤Ÿæ€ä¹ˆåŠï¼Ÿ

**A**: ä½¿ç”¨åˆ†å—å¤„ç†ï¼š

```python
def process_large_dataset_chunked(filepath, chunk_size=365*5):
    """
    åˆ†å—å¤„ç†å¤§å‹ NetCDF æ–‡ä»¶

    Parameters
    ----------
    filepath : str
        NetCDF æ–‡ä»¶è·¯å¾„
    chunk_size : int
        æ¯å—çš„æ—¶é—´æ­¥æ•°ï¼ˆé»˜è®¤5å¹´ï¼‰

    Yields
    ------
    chunk_result : dict
        æ¯å—çš„å¤„ç†ç»“æœ
    """
    ds = xr.open_dataset(filepath, chunks={'time': chunk_size})

    n_chunks = len(ds.time) // chunk_size + 1

    for i in range(n_chunks):
        start_idx = i * chunk_size
        end_idx = min((i + 1) * chunk_size, len(ds.time))

        # åŠ è½½å½“å‰å—åˆ°å†…å­˜
        chunk = ds.isel(time=slice(start_idx, end_idx)).load()

        # å¤„ç†...
        et0_chunk = calculate_et0(...)
        extreme_mask_chunk = detect_extreme_events_hist(et0_chunk)

        yield {
            'time': chunk.time.values,
            'et0': et0_chunk,
            'extreme_mask': extreme_mask_chunk
        }

        # æ¸…ç†å†…å­˜
        del chunk
        import gc
        gc.collect()
```

### Q4: å¦‚ä½•éªŒè¯æˆ‘çš„æ•°æ®å¤„ç†æ˜¯å¦æ­£ç¡®ï¼Ÿ

**A**: ä½¿ç”¨ä»¥ä¸‹æ£€æŸ¥æ¸…å•ï¼š

```python
def validate_processed_data(df):
    """
    éªŒè¯å¤„ç†åçš„æ•°æ®è´¨é‡

    Returns
    -------
    is_valid : bool
        æ•°æ®æ˜¯å¦é€šè¿‡æ‰€æœ‰æ£€æŸ¥
    """
    checks = []

    # 1. æ¸©åº¦é€»è¾‘æ€§
    checks.append(
        np.all(df['T_min'] <= df['T_mean']) and
        np.all(df['T_mean'] <= df['T_max'])
    )

    # 2. ç‰©ç†èŒƒå›´
    checks.append(np.all((df['Rs'] >= 0) & (df['Rs'] <= 40)))
    checks.append(np.all((df['u2'] >= 0) & (df['u2'] <= 30)))
    checks.append(np.all((df['ea'] >= 0) & (df['ea'] <= 7)))

    # 3. ç¼ºå¤±å€¼æ¯”ä¾‹
    missing_rate = df.isnull().sum() / len(df)
    checks.append(np.all(missing_rate < 0.1))  # <10% ç¼ºå¤±

    # 4. æ—¶é—´è¿ç»­æ€§
    time_diff = df.index.to_series().diff()
    checks.append(np.all(time_diff[1:] == pd.Timedelta(days=1)))

    # 5. åˆç†çš„å­£èŠ‚æ€§
    monthly_mean = df['T_mean'].groupby(df.index.month).mean()
    seasonal_range = monthly_mean.max() - monthly_mean.min()
    checks.append(seasonal_range > 5)  # è‡³å°‘5Â°Cå­£èŠ‚å·®å¼‚

    is_valid = all(checks)

    if not is_valid:
        print("æ•°æ®éªŒè¯å¤±è´¥ï¼")
        for i, check in enumerate(checks, 1):
            status = "âœ“" if check else "âœ—"
            print(f"  æ£€æŸ¥ {i}: {status}")

    return is_valid
```

---

## 10. å‚è€ƒèµ„æº

### æ•°æ®æº

1. **ERA5-Land**: https://cds.climate.copernicus.eu/
2. **gridMET**: https://www.climatologylab.org/gridmet.html
3. **PRISM**: https://prism.oregonstate.edu/
4. **Daymet**: https://daymet.ornl.gov/

### å·¥å…·æ–‡æ¡£

1. **xarray**: https://docs.xarray.dev/
2. **pandas**: https://pandas.pydata.org/docs/
3. **netCDF4-python**: https://unidata.github.io/netcdf4-python/

### æ¨èé˜…è¯»

1. **Hersbach et al. (2020)**. The ERA5 global reanalysis. *Quarterly Journal of the Royal Meteorological Society*.
2. **Abatzoglou, J. T. (2013)**. Development of gridded surface meteorological data for ecological applications and modelling. *International Journal of Climatology*.
