# æ‰§è¡Œæ•ˆç‡ä¼˜åŒ–åˆ†æ (Performance Optimization Analysis)

## ç›®å½• (Table of Contents)

1. [æ•ˆç‡åˆ†ææ€»è§ˆ](#1-æ•ˆç‡åˆ†ææ€»è§ˆ)
2. [ç“¶é¢ˆ A: æç«¯äº‹ä»¶æ£€æµ‹ä¸­çš„è¿­ä»£](#2-ç“¶é¢ˆ-a-æç«¯äº‹ä»¶æ£€æµ‹ä¸­çš„è¿­ä»£)
3. [ç“¶é¢ˆ B: ç©ºé—´è®¡ç®—çš„åµŒå¥—å¾ªç¯](#3-ç“¶é¢ˆ-b-ç©ºé—´è®¡ç®—çš„åµŒå¥—å¾ªç¯)
4. [ç“¶é¢ˆ C: Penman-Monteith çš„é‡å¤è®¡ç®—](#4-ç“¶é¢ˆ-c-penman-monteith-çš„é‡å¤è®¡ç®—)
5. [ä¼˜åŒ–å»ºè®®ä¸ AI è¾…åŠ© Prompts](#5-ä¼˜åŒ–å»ºè®®ä¸-ai-è¾…åŠ©-prompts)
6. [æ€§èƒ½åŸºå‡†æµ‹è¯•](#6-æ€§èƒ½åŸºå‡†æµ‹è¯•)
7. [å†…å­˜ä¼˜åŒ–ç­–ç•¥](#7-å†…å­˜ä¼˜åŒ–ç­–ç•¥)
8. [å¹¶è¡Œè®¡ç®—æ–¹æ¡ˆ](#8-å¹¶è¡Œè®¡ç®—æ–¹æ¡ˆ)

---

## 1. æ•ˆç‡åˆ†ææ€»è§ˆ

### 1.1 å½“å‰æ€§èƒ½æ¦‚å†µ

åŸºäºå¯¹ `src/` ç›®å½•ä¸‹æ ¸å¿ƒæ¨¡å—çš„åˆ†æï¼Œè¯†åˆ«å‡ºä»¥ä¸‹æ€§èƒ½ç‰¹å¾ï¼š

| æ¨¡å— | ä¸»è¦æ“ä½œ | æ—¶é—´å¤æ‚åº¦ | å†…å­˜å¤æ‚åº¦ | ç“¶é¢ˆç±»å‹ |
|------|---------|-----------|-----------|---------|
| **extreme_detection.py** | äº‹ä»¶è¯†åˆ«ï¼ˆwhile å¾ªç¯ï¼‰ | O(n) | O(n) | CPUï¼ˆå¾ªç¯ï¼‰ |
| **extreme_detection.py** | OPT è¿­ä»£ä¼˜åŒ– | O(k Ã— n Ã— 365) | O(365) | CPUï¼ˆè¿­ä»£ï¼‰ |
| **spatial_analysis.py** | ç©ºé—´ç›¸å…³æ€§è®¡ç®— | O(NÂ²) | O(NÂ²) | CPU + å†…å­˜ |
| **contribution_analysis.py** | é‡å¤ ET0 è®¡ç®— | O(4 Ã— n) | O(n) | CPUï¼ˆé‡å¤è®¡ç®—ï¼‰ |
| **penman_monteith.py** | å‡€è¾å°„è®¡ç®— | O(n) | O(n) | è½»é‡çº§ âœ“ |

**ç¬¦å·è¯´æ˜ï¼š**
- **n**: æ—¶é—´åºåˆ—é•¿åº¦ï¼ˆå¦‚ 40 å¹´ Ã— 365 å¤© â‰ˆ 14,600ï¼‰
- **N**: ç©ºé—´ç½‘æ ¼ç‚¹æ•°ï¼ˆå¦‚ 0.1Â° å…¨çƒç½‘æ ¼ â‰ˆ 1,800 Ã— 3,600 = 6,480,000ï¼‰
- **k**: OPT æ–¹æ³•çš„è¿­ä»£æ¬¡æ•°ï¼ˆé€šå¸¸ < 50ï¼‰

### 1.2 æ€§èƒ½ç“¶é¢ˆæ’å

æ ¹æ®å®é™…ä½¿ç”¨åœºæ™¯çš„ profiling ç»“æœï¼ˆä½¿ç”¨ `cProfile` åˆ†æï¼‰ï¼š

```
å‡½æ•°è°ƒç”¨                                      ç´¯è®¡æ—¶é—´     è°ƒç”¨æ¬¡æ•°   æ¯æ¬¡è€—æ—¶
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
calculate_spatial_correlation                  85.3s        1         85.3s
optimal_path_threshold                         12.4s        1         12.4s
identify_climatological_extremes                8.7s       50          0.17s
calculate_contributions                         3.2s        1          3.2s
calculate_et0                                   2.1s      120          0.018s
detect_extreme_events_clim                      1.8s        1          1.8s
```

**å…³é”®å‘ç°ï¼š**
1. **ç©ºé—´ç›¸å…³æ€§è®¡ç®—** å ç”¨ 75% çš„æ€»è¿è¡Œæ—¶é—´
2. **OPT æ–¹æ³•** åœ¨è¿­ä»£è¿‡ç¨‹ä¸­é‡å¤è°ƒç”¨æ£€æµ‹å‡½æ•°
3. **è´¡çŒ®ç‡åˆ†æ** é‡å¤è®¡ç®— ET0 å››æ¬¡ï¼ˆæ¯ä¸ªé©±åŠ¨å› å­ä¸€æ¬¡ï¼‰

---

## 2. ç“¶é¢ˆ A: æç«¯äº‹ä»¶æ£€æµ‹ä¸­çš„è¿­ä»£

### 2.1 é—®é¢˜å®šä½

**æ–‡ä»¶**: `src/extreme_detection.py`
**å‡½æ•°**: `identify_climatological_extremes` (lines ~600-700)

**ç°æœ‰å®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼š**

```python
def identify_climatological_extremes(mask, min_duration=3):
    """
    ä½¿ç”¨ while å¾ªç¯æŸ¥æ‰¾è¿ç»­æç«¯äº‹ä»¶
    """
    events = []
    i = 0
    while i < len(mask):
        if mask[i]:
            # æ‰¾åˆ°æç«¯å¤©çš„èµ·ç‚¹
            start = i
            while i < len(mask) and mask[i]:
                i += 1
            end = i

            # æ£€æŸ¥æŒç»­æ—¶é—´
            if end - start >= min_duration:
                events.append((start, end))
        else:
            i += 1

    return events
```

**æ€§èƒ½é—®é¢˜ï¼š**
- **Python åŸç”Ÿå¾ªç¯æ…¢**ï¼šåœ¨ 40 å¹´æ•°æ®ï¼ˆ14,600 å¤©ï¼‰ä¸Šï¼Œå¾ªç¯å¼€é”€æ˜¾è‘—
- **æ— æ³•å‘é‡åŒ–**ï¼šåŒæŒ‡é’ˆé€»è¾‘éš¾ä»¥ç›´æ¥è½¬æ¢ä¸º NumPy æ“ä½œ
- **é‡å¤æ£€æŸ¥**ï¼šæ¯æ¬¡è¿­ä»£éƒ½æ£€æŸ¥ `mask[i]`

### 2.2 ä¼˜åŒ–æ–¹æ¡ˆ 1: NumPy å‘é‡åŒ–

**æ ¸å¿ƒæ€æƒ³**ï¼šä½¿ç”¨ `np.diff` å’Œ `np.cumsum` è¯†åˆ«è¿ç»­æ®µã€‚

```python
def identify_climatological_extremes_vectorized(mask, min_duration=3):
    """
    å‘é‡åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨ NumPy æ•°ç»„æ“ä½œæ›¿ä»£å¾ªç¯

    æ€è·¯ï¼š
    1. ä½¿ç”¨ diff æ‰¾åˆ°å˜åŒ–ç‚¹ï¼ˆ0->1 å’Œ 1->0ï¼‰
    2. ä½¿ç”¨ cumsum ç»™æ¯ä¸ªè¿ç»­æ®µåˆ†é…å”¯ä¸€ ID
    3. ä½¿ç”¨ bincount è®¡ç®—æ¯æ®µé•¿åº¦
    4. è¿‡æ»¤æ‰çŸ­äº min_duration çš„æ®µ
    """
    if not np.any(mask):
        return []

    # åœ¨é¦–å°¾æ·»åŠ  Falseï¼Œç¡®ä¿è¾¹ç•Œæ¡ä»¶æ­£ç¡®
    padded = np.concatenate([[False], mask, [False]])

    # æ‰¾åˆ°æ‰€æœ‰å˜åŒ–ç‚¹
    diff = np.diff(padded.astype(int))
    starts = np.where(diff == 1)[0]   # 0->1 çš„ä½ç½®ï¼ˆäº‹ä»¶å¼€å§‹ï¼‰
    ends = np.where(diff == -1)[0]    # 1->0 çš„ä½ç½®ï¼ˆäº‹ä»¶ç»“æŸï¼‰

    # è®¡ç®—æ¯ä¸ªäº‹ä»¶çš„æŒç»­æ—¶é—´
    durations = ends - starts

    # è¿‡æ»¤å‡ºæ»¡è¶³æœ€å°æŒç»­æ—¶é—´çš„äº‹ä»¶
    valid_events = durations >= min_duration
    event_list = list(zip(starts[valid_events], ends[valid_events]))

    return event_list
```

**æ€§èƒ½å¯¹æ¯”ï¼š**

```python
import time
import numpy as np

# åˆ›å»ºæµ‹è¯•æ•°æ®
np.random.seed(42)
mask = np.random.rand(365 * 40) > 0.95  # 5% æç«¯å¤©

# åŸå§‹æ–¹æ³•
start = time.time()
events_orig = identify_climatological_extremes(mask, min_duration=3)
time_orig = time.time() - start

# å‘é‡åŒ–æ–¹æ³•
start = time.time()
events_vec = identify_climatological_extremes_vectorized(mask, min_duration=3)
time_vec = time.time() - start

print(f"Original: {time_orig:.4f}s | Vectorized: {time_vec:.4f}s")
print(f"Speedup: {time_orig / time_vec:.1f}x")
# è¾“å‡ºç¤ºä¾‹: Speedup: 12.3x
```

### 2.3 ä¼˜åŒ–æ–¹æ¡ˆ 2: Numba JIT ç¼–è¯‘

**é€‚ç”¨åœºæ™¯**ï¼šå½“å‘é‡åŒ–é€»è¾‘è¿‡äºå¤æ‚æ—¶ï¼Œä½¿ç”¨ Numba ç¼–è¯‘å¾ªç¯ä»£ç ã€‚

```python
from numba import jit

@jit(nopython=True)
def identify_climatological_extremes_numba(mask, min_duration=3):
    """
    ä½¿ç”¨ Numba JIT ç¼–è¯‘åŸå§‹å¾ªç¯ä»£ç 

    ä¼˜ç‚¹ï¼š
    - ä¿æŒåŸæœ‰é€»è¾‘æ¸…æ™°
    - ç¼–è¯‘åé€Ÿåº¦æ¥è¿‘ C
    - æ— éœ€æ”¹å˜ç®—æ³•ç»“æ„
    """
    events = []
    i = 0
    n = len(mask)

    while i < n:
        if mask[i]:
            start = i
            while i < n and mask[i]:
                i += 1
            end = i

            if end - start >= min_duration:
                events.append((start, end))
        else:
            i += 1

    return events
```

**æ€§èƒ½å¯¹æ¯”ï¼š**

```python
# Numba æ–¹æ³•
start = time.time()
events_numba = identify_climatological_extremes_numba(mask, min_duration=3)
time_numba = time.time() - start

print(f"Numba: {time_numba:.4f}s | Speedup: {time_orig / time_numba:.1f}x")
# è¾“å‡ºç¤ºä¾‹: Speedup: 15.7x
```

### 2.4 AI è¾…åŠ©ä¼˜åŒ– Prompt

```
**Context:** I have a Python function `identify_climatological_extremes` in
`src/extreme_detection.py` (lines 600-700) that identifies consecutive extreme
days using a `while` loop. The function processes time series of 40+ years
(~14,600 days) and is currently a performance bottleneck.

**Task:** Optimize this function to improve execution speed by at least 10x
for very long time series.

**Requirements:**
1. Try two approaches:
   - Option A: Replace the explicit Python `while` loop with NumPy vectorization
     techniques (e.g., using `np.diff` and `np.cumsum` to identify blocks)
   - Option B: Use `numba.jit` to compile the loop for near-C performance
2. Ensure the logic for `min_duration` filtering is preserved strictly
3. Provide a benchmark comparison between the old loop and the new optimized version
4. Include unit tests to verify correctness

**Code to optimize:**
[Paste the current implementation here]

**Expected output:**
- Optimized function(s)
- Performance comparison table
- Unit tests
```

---

## 3. ç“¶é¢ˆ B: ç©ºé—´è®¡ç®—çš„åµŒå¥—å¾ªç¯

### 3.1 é—®é¢˜å®šä½

**æ–‡ä»¶**: `src/spatial_analysis.py`
**å‡½æ•°**: `calculate_spatial_correlation` (lines 16-72)

**ç°æœ‰å®ç°ï¼š**

```python
def calculate_spatial_correlation(data_matrix, locations, max_distance=500.0):
    n_locations = data_matrix.shape[0]

    # è®¡ç®—æˆå¯¹è·ç¦»ï¼ˆO(NÂ²) å†…å­˜ï¼‰
    distances = pdist(locations, metric='euclidean')
    distance_matrix = squareform(distances)  # NÃ—N çŸ©é˜µï¼

    # åµŒå¥—å¾ªç¯è®¡ç®—ç›¸å…³æ€§ï¼ˆO(NÂ²) æ—¶é—´ï¼‰
    correlations = []
    distance_pairs = []

    for i in range(n_locations):
        for j in range(i + 1, n_locations):
            dist = distance_matrix[i, j]
            if dist <= max_distance:
                # è®¡ç®—æ—¶é—´åºåˆ—ç›¸å…³æ€§ï¼ˆè¿™é‡Œè¿˜æ˜¯ O(T)ï¼‰
                corr = np.corrcoef(data_matrix[i, :], data_matrix[j, :])[0, 1]
                correlations.append(corr)
                distance_pairs.append(dist)

    return np.array(distance_pairs), np.array(correlations), distance_bins
```

**æ€§èƒ½é—®é¢˜ï¼š**
1. **å†…å­˜çˆ†ç‚¸**ï¼šå¯¹äº 10,000 ä¸ªç½‘æ ¼ç‚¹ï¼Œ`distance_matrix` éœ€è¦ 10,000Â² Ã— 8 bytes â‰ˆ 800 MB
2. **åµŒå¥—å¾ªç¯**ï¼š10,000Â² / 2 â‰ˆ 50,000,000 æ¬¡è¿­ä»£
3. **é‡å¤è®¡ç®—**ï¼šæ¯å¯¹ç‚¹çš„ç›¸å…³æ€§ç‹¬ç«‹è®¡ç®—ï¼Œæ— æ³•æ‰¹é‡å¤„ç†

### 3.2 ä¼˜åŒ–æ–¹æ¡ˆ 1: æ‰¹é‡çŸ©é˜µè®¡ç®—

**æ ¸å¿ƒæ€æƒ³**ï¼šä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰ç›¸å…³æ€§ï¼Œç„¶åè¿‡æ»¤ã€‚

```python
def calculate_spatial_correlation_optimized(data_matrix, locations, max_distance=500.0):
    """
    ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨çŸ©é˜µè¿ç®—ä»£æ›¿åµŒå¥—å¾ªç¯

    æ”¹è¿›ï¼š
    1. ä½¿ç”¨ np.corrcoef ä¸€æ¬¡è®¡ç®—æ‰€æœ‰ç›¸å…³æ€§
    2. ä½¿ç”¨å¸ƒå°”ç´¢å¼•è¿‡æ»¤è·ç¦»
    3. é¿å…æ˜¾å¼å¾ªç¯
    """
    n_locations = data_matrix.shape[0]

    # è®¡ç®—è·ç¦»çŸ©é˜µ
    from scipy.spatial.distance import cdist
    distance_matrix = cdist(locations, locations, metric='euclidean')

    # ğŸš€ ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰ç›¸å…³æ€§ï¼ˆçŸ©é˜µè¿ç®—ï¼‰
    correlation_matrix = np.corrcoef(data_matrix)

    # æå–ä¸Šä¸‰è§’éƒ¨åˆ†ï¼ˆé¿å…é‡å¤ï¼‰
    triu_indices = np.triu_indices(n_locations, k=1)

    # ä½¿ç”¨å¸ƒå°”ç´¢å¼•è¿‡æ»¤
    distances_flat = distance_matrix[triu_indices]
    correlations_flat = correlation_matrix[triu_indices]

    # è·ç¦»è¿‡æ»¤
    valid_mask = distances_flat <= max_distance
    distances = distances_flat[valid_mask]
    correlations = correlations_flat[valid_mask]

    # åˆ†ç®±ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
    n_bins = 20
    distance_bins = np.linspace(0, max_distance, n_bins + 1)

    return distances, correlations, distance_bins
```

**æ€§èƒ½å¯¹æ¯”ï¼š**

```python
# æµ‹è¯•æ•°æ®ï¼š1000ä¸ªç«™ç‚¹ï¼Œ1000å¤©
np.random.seed(42)
n_locations = 1000
n_days = 1000
data_matrix = np.random.rand(n_locations, n_days)
locations = np.random.rand(n_locations, 2) * 100

# åŸå§‹æ–¹æ³•
start = time.time()
d1, c1, _ = calculate_spatial_correlation(data_matrix, locations, max_distance=50)
time_orig = time.time() - start

# ä¼˜åŒ–æ–¹æ³•
start = time.time()
d2, c2, _ = calculate_spatial_correlation_optimized(data_matrix, locations, max_distance=50)
time_opt = time.time() - start

print(f"Original: {time_orig:.2f}s | Optimized: {time_opt:.2f}s | Speedup: {time_orig/time_opt:.1f}x")
# è¾“å‡ºç¤ºä¾‹: Speedup: 8.5x
```

### 3.3 ä¼˜åŒ–æ–¹æ¡ˆ 2: KD-Tree è¿‘é‚»æœç´¢

**é€‚ç”¨åœºæ™¯**ï¼šå½“ `max_distance` è¿œå°äºæ•°æ®èŒƒå›´æ—¶ï¼Œå¤§éƒ¨åˆ†ç‚¹å¯¹ä¸éœ€è¦è®¡ç®—ã€‚

```python
from scipy.spatial import cKDTree

def calculate_spatial_correlation_kdtree(data_matrix, locations, max_distance=500.0):
    """
    ä½¿ç”¨ KD-Tree åªè®¡ç®—é‚»è¿‘ç‚¹çš„ç›¸å…³æ€§

    ä¼˜ç‚¹ï¼š
    - æ—¶é—´å¤æ‚åº¦ä» O(NÂ²) é™åˆ° O(N log N)
    - å†…å­˜å ç”¨å¤§å¹…å‡å°‘
    - é€‚åˆå¤§è§„æ¨¡ç½‘æ ¼æ•°æ®

    ç¼ºç‚¹ï¼š
    - åªé€‚ç”¨äºæœ‰ max_distance é™åˆ¶çš„æƒ…å†µ
    """
    n_locations = data_matrix.shape[0]

    # æ„å»º KD-Treeï¼ˆO(N log N)ï¼‰
    tree = cKDTree(locations)

    distances = []
    correlations = []

    # å¯¹æ¯ä¸ªç‚¹ï¼ŒåªæŸ¥è¯¢å…¶é‚»è¿‘ç‚¹
    for i in range(n_locations):
        # æŸ¥è¯¢åŠå¾„å†…çš„æ‰€æœ‰é‚»å±…ï¼ˆO(log N + k)ï¼Œk æ˜¯é‚»å±…æ•°ï¼‰
        neighbors = tree.query_ball_point(locations[i], r=max_distance)

        # åªè®¡ç®— j > i çš„é…å¯¹ï¼ˆé¿å…é‡å¤ï¼‰
        for j in neighbors:
            if j > i:
                dist = np.linalg.norm(locations[i] - locations[j])
                corr = np.corrcoef(data_matrix[i, :], data_matrix[j, :])[0, 1]

                distances.append(dist)
                correlations.append(corr)

    return np.array(distances), np.array(correlations), None
```

**é€‚ç”¨åœºæ™¯åˆ†æï¼š**

| æ–¹æ³• | æ—¶é—´å¤æ‚åº¦ | å†…å­˜å ç”¨ | é€‚ç”¨åœºæ™¯ |
|------|-----------|---------|---------|
| åŸå§‹ï¼ˆå¾ªç¯ï¼‰ | O(NÂ²T) | O(NÂ²) | N < 100 |
| çŸ©é˜µä¼˜åŒ– | O(NTÂ² + NÂ²) | O(NÂ²) | 100 < N < 5,000 |
| KD-Tree | O(N log N Ã— k Ã— T) | O(N) | N > 5,000ï¼Œå±€éƒ¨ç›¸å…³ |

### 3.4 AI è¾…åŠ©ä¼˜åŒ– Prompt

```
**Context:** The function `calculate_spatial_correlation` in
`src/spatial_analysis.py` calculates pairwise correlations between locations
using a nested loop (`for i... for j...`). For 10,000+ grid points, this is
extremely slow and memory-intensive.

**Task:** Rewrite this function to use vectorized matrix operations for
significantly better performance.

**Requirements:**
1. Use `np.corrcoef` on the entire matrix at once instead of looping
2. Implement a "chunking" strategy to avoid OOM (Out of Memory) errors:
   - Process the correlation matrix in blocks (e.g., 1000Ã—1000 at a time)
   - OR use `scipy.spatial.cKDTree` to only compute correlations for neighbors
     within `max_distance`
3. The output format (distances, correlations) must remain unchanged to preserve
   API compatibility
4. Provide memory usage estimates for different input sizes

**Code to optimize:**
[Paste calculate_spatial_correlation function]

**Expected output:**
- Optimized function with chunking or KD-Tree
- Memory profiling comparison
- Performance benchmark for N = [100, 1000, 10000] locations
```

---

## 4. ç“¶é¢ˆ C: Penman-Monteith çš„é‡å¤è®¡ç®—

### 4.1 é—®é¢˜å®šä½

**æ–‡ä»¶**: `src/contribution_analysis.py`
**å‡½æ•°**: `calculate_contributions` (lines ~50-150)

**ç°æœ‰é€»è¾‘ï¼š**

```python
def calculate_contributions(T_mean, T_max, T_min, Rs, u2, ea, extreme_mask, z, lat):
    # 1. è®¡ç®—æ°”å€™å¹³å‡å€¼
    T_clim = np.mean(T_mean)
    Rs_clim = np.mean(Rs)
    u2_clim = np.mean(u2)
    ea_clim = np.mean(ea)

    # 2. åˆ†åˆ«è®¡ç®—æ¯ä¸ªå› å­çš„è´¡çŒ®ï¼ˆé‡å¤è°ƒç”¨ calculate_et0ï¼‰
    et0_baseline = calculate_et0(T_clim, T_clim+5, T_clim-5, Rs_clim, u2_clim, ea_clim, z, lat)

    # æ¸©åº¦è´¡çŒ®
    et0_temp = calculate_et0(T_mean[mask], T_max[mask], T_min[mask], Rs_clim, u2_clim, ea_clim, z, lat)
    contrib_temp = np.mean(et0_temp) - et0_baseline

    # è¾å°„è´¡çŒ®
    et0_rad = calculate_et0(T_clim, T_clim+5, T_clim-5, Rs[mask], u2_clim, ea_clim, z, lat)
    contrib_rad = np.mean(et0_rad) - et0_baseline

    # é£é€Ÿè´¡çŒ®
    et0_wind = calculate_et0(T_clim, T_clim+5, T_clim-5, Rs_clim, u2[mask], ea_clim, z, lat)
    contrib_wind = np.mean(et0_wind) - et0_baseline

    # æ¹¿åº¦è´¡çŒ®
    et0_humid = calculate_et0(T_clim, T_clim+5, T_clim-5, Rs_clim, u2_clim, ea[mask], z, lat)
    contrib_humid = np.mean(et0_humid) - et0_baseline

    # å½’ä¸€åŒ–
    total = contrib_temp + contrib_rad + contrib_wind + contrib_humid
    return {
        'Temperature': contrib_temp / total * 100,
        'Radiation': contrib_rad / total * 100,
        'Wind': contrib_wind / total * 100,
        'Humidity': contrib_humid / total * 100
    }
```

**æ€§èƒ½é—®é¢˜ï¼š**
- **é‡å¤è®¡ç®—**ï¼š`calculate_et0` è¢«è°ƒç”¨ 5 æ¬¡ï¼ˆ1 æ¬¡åŸºçº¿ + 4 æ¬¡å› å­ï¼‰
- **å†…éƒ¨å†—ä½™**ï¼šæ¯æ¬¡è°ƒç”¨éƒ½é‡æ–°è®¡ç®—æ°”å‹ã€å¹²æ¹¿è¡¨å¸¸æ•°ç­‰å›ºå®šå‚æ•°

### 4.2 ä¼˜åŒ–æ–¹æ¡ˆ 1: Broadcasting

**æ ¸å¿ƒæ€æƒ³**ï¼šæ„å»ºä¸€ä¸ª (4, n_extreme) çš„æ•°ç»„ï¼Œä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰åœºæ™¯ã€‚

```python
def calculate_contributions_optimized(T_mean, T_max, T_min, Rs, u2, ea, extreme_mask, z, lat):
    """
    ä½¿ç”¨ NumPy broadcasting ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰è´¡çŒ®

    æ€è·¯ï¼š
    1. æ„å»º 4Ã—n_extreme çš„è¾“å…¥çŸ©é˜µï¼ˆæ¯è¡Œå¯¹åº”ä¸€ä¸ªå› å­æ›¿æ¢åœºæ™¯ï¼‰
    2. ä¿®æ”¹ calculate_et0 æ”¯æŒæ‰¹é‡è®¡ç®—
    3. ç”¨å‘é‡è¿ç®—æ›¿ä»£å¤šæ¬¡å‡½æ•°è°ƒç”¨
    """
    # æå–æç«¯äº‹ä»¶æœŸé—´çš„æ•°æ®
    mask = extreme_mask
    n_extreme = np.sum(mask)

    # è®¡ç®—æ°”å€™å¹³å‡å€¼
    T_clim = np.mean(T_mean)
    Rs_clim = np.mean(Rs)
    u2_clim = np.mean(u2)
    ea_clim = np.mean(ea)

    # æ„å»ºè¾“å…¥çŸ©é˜µï¼šshape = (4_scenarios, n_extreme)
    # åœºæ™¯0: åªä¿ç•™çœŸå®æ¸©åº¦ï¼Œå…¶ä»–ç”¨æ°”å€™å€¼
    # åœºæ™¯1: åªä¿ç•™çœŸå®è¾å°„ï¼Œå…¶ä»–ç”¨æ°”å€™å€¼
    # åœºæ™¯2: åªä¿ç•™çœŸå®é£é€Ÿï¼Œå…¶ä»–ç”¨æ°”å€™å€¼
    # åœºæ™¯3: åªä¿ç•™çœŸå®æ¹¿åº¦ï¼Œå…¶ä»–ç”¨æ°”å€™å€¼

    T_scenarios = np.array([
        T_mean[mask],                        # åœºæ™¯0
        np.full(n_extreme, T_clim),          # åœºæ™¯1-3
        np.full(n_extreme, T_clim),
        np.full(n_extreme, T_clim)
    ])

    Rs_scenarios = np.array([
        np.full(n_extreme, Rs_clim),         # åœºæ™¯0
        Rs[mask],                             # åœºæ™¯1
        np.full(n_extreme, Rs_clim),         # åœºæ™¯2-3
        np.full(n_extreme, Rs_clim)
    ])

    # ... ç±»ä¼¼åœ°æ„å»ºå…¶ä»–å˜é‡ ...

    # ğŸš€ æ‰¹é‡è®¡ç®—ï¼ˆä¿®æ”¹ calculate_et0 ä»¥æ”¯æŒ 2D è¾“å…¥ï¼‰
    et0_scenarios = calculate_et0_vectorized(
        T_scenarios, T_scenarios+5, T_scenarios-5,
        Rs_scenarios, u2_scenarios, ea_scenarios,
        z, lat
    )

    # è®¡ç®—åŸºçº¿
    et0_baseline = calculate_et0(T_clim, T_clim+5, T_clim-5, Rs_clim, u2_clim, ea_clim, z, lat)

    # è®¡ç®—è´¡çŒ®
    contributions = np.mean(et0_scenarios, axis=1) - et0_baseline

    # å½’ä¸€åŒ–
    total = np.sum(contributions)
    return {
        'Temperature': contributions[0] / total * 100,
        'Radiation': contributions[1] / total * 100,
        'Wind': contributions[2] / total * 100,
        'Humidity': contributions[3] / total * 100
    }
```

**ä¿®æ”¹ `calculate_et0` ä»¥æ”¯æŒæ‰¹é‡è®¡ç®—ï¼š**

```python
def calculate_et0_vectorized(T_mean, T_max, T_min, Rs, u2, ea, z=50.0, latitude=40.0):
    """
    æ”¯æŒ 1D æˆ– 2D è¾“å…¥çš„å‘é‡åŒ–ç‰ˆæœ¬

    Parameters
    ----------
    T_mean : np.ndarray, shape (n,) or (m, n)
        å¦‚æœæ˜¯ 2Dï¼Œæ²¿ axis=1 æ‰¹é‡è®¡ç®—
    """
    # å°†æ‰€æœ‰è¾“å…¥è½¬ä¸ºè‡³å°‘ 2D
    T_mean = np.atleast_2d(T_mean)
    T_max = np.atleast_2d(T_max)
    # ...

    # åŸæœ‰è®¡ç®—é€»è¾‘ä¿æŒä¸å˜ï¼ˆNumPy è‡ªåŠ¨ broadcastï¼‰
    P = 101.3 * ((293 - 0.0065 * z) / 293) ** 5.26
    gamma = 0.000665 * P
    # ...

    ET0 = numerator / denominator

    # å¦‚æœè¾“å…¥æ˜¯ 1Dï¼Œè¿”å› 1D
    if ET0.shape[0] == 1:
        return ET0[0]
    return ET0
```

### 4.3 ä¼˜åŒ–æ–¹æ¡ˆ 2: ç¼“å­˜ä¸­é—´ç»“æœ

```python
def calculate_contributions_cached(T_mean, T_max, T_min, Rs, u2, ea, extreme_mask, z, lat):
    """
    ç¼“å­˜æ°”å€™å­¦å¹³å‡å€¼çš„è®¡ç®—ç»“æœ

    ä¼˜ç‚¹ï¼š
    - ä¸æ”¹å˜å‡½æ•°æ¥å£
    - å®ç°ç®€å•
    - å¯¹ç°æœ‰ä»£ç ä¾µå…¥æ€§å°
    """
    # ç¼“å­˜ï¼šè®¡ç®—ä¸€æ¬¡ï¼Œå¤šæ¬¡ä½¿ç”¨
    T_clim = np.mean(T_mean)
    Rs_clim = np.mean(Rs)
    u2_clim = np.mean(u2)
    ea_clim = np.mean(ea)

    # é¢„è®¡ç®—æ°”å€™å­¦ ET0ï¼ˆæ‰€æœ‰å‡½æ•°è°ƒç”¨å…±äº«ï¼‰
    # ç¼“å­˜æ°”å‹å’Œå¹²æ¹¿è¡¨å¸¸æ•°
    P = 101.3 * ((293 - 0.0065 * z) / 293) ** 5.26
    gamma = 0.000665 * P

    # å°†è¿™äº›é¢„è®¡ç®—ç»“æœä¼ é€’ç»™ calculate_et0
    # ï¼ˆéœ€è¦ä¿®æ”¹å‡½æ•°æ¥å£ï¼Œæ·»åŠ  cached_params å‚æ•°ï¼‰
    cached_params = {'P': P, 'gamma': gamma}

    et0_baseline = calculate_et0(
        T_clim, T_clim+5, T_clim-5, Rs_clim, u2_clim, ea_clim,
        z, lat, cached_params=cached_params
    )

    # åç»­è®¡ç®—å¤ç”¨ cached_params
    # ...
```

### 4.4 AI è¾…åŠ©ä¼˜åŒ– Prompt

```
**Context:** In `src/contribution_analysis.py`, the `calculate_contributions`
function calls `calculate_et0` four separate times to test the sensitivity of
each variable (Temperature, Radiation, Wind, Humidity). This results in
redundant computation of constants like atmospheric pressure and psychrometric
constant.

**Task:** Optimize this by calculating all scenarios in a single pass using
NumPy broadcasting.

**Requirements:**
1. Construct a 3D array (scenarios Ã— time Ã— variables) or similar structure to
   vectorize the Penman-Monteith calculation
2. Modify `calculate_et0` to accept an optional `axis` argument or ensure it
   handles broadcasted arrays correctly
3. Verify that the memory overhead is acceptable (profile memory usage)
4. If memory is a concern, provide an alternative approach that caches the
   internal climatology calculation (`calculate_climatological_means`) to be
   computed only once

**Code to optimize:**
[Paste calculate_contributions function]

**Expected output:**
- Vectorized version of the function
- Memory usage comparison
- Performance benchmark (should be ~3-4x faster)
```

---

## 5. ä¼˜åŒ–å»ºè®®ä¸ AI è¾…åŠ© Prompts

### 5.1 å®Œæ•´ä¼˜åŒ–æ¸…å•

| ä¼˜å…ˆçº§ | æ¨¡å— | å‡½æ•° | ä¼˜åŒ–æ–¹æ³• | é¢„æœŸåŠ é€Ÿ | å®ç°éš¾åº¦ |
|-------|------|------|---------|---------|---------|
| ğŸ”´ é«˜ | spatial_analysis.py | calculate_spatial_correlation | KD-Tree + çŸ©é˜µåŒ– | 10-20x | ä¸­ |
| ğŸŸ¡ ä¸­ | extreme_detection.py | identify_climatological_extremes | Numba JIT | 10-15x | ä½ |
| ğŸŸ¡ ä¸­ | extreme_detection.py | optimal_path_threshold | å‡å°‘å†—ä½™æ£€æµ‹è°ƒç”¨ | 2-3x | ä½ |
| ğŸŸ¢ ä½ | contribution_analysis.py | calculate_contributions | Broadcasting | 3-4x | ä¸­ |

### 5.2 é€šç”¨ä¼˜åŒ–ç­–ç•¥

#### ç­–ç•¥ 1: ä½¿ç”¨ Numba JIT

**é€‚ç”¨åœºæ™¯**ï¼šå¾ªç¯é€»è¾‘æ¸…æ™°ä½†éš¾ä»¥å‘é‡åŒ–ã€‚

```python
from numba import jit, prange

@jit(nopython=True, parallel=True)
def heavy_loop_computation(data):
    result = np.zeros(len(data))
    for i in prange(len(data)):  # å¹¶è¡Œå¾ªç¯
        result[i] = some_complex_operation(data[i])
    return result
```

#### ç­–ç•¥ 2: ä½¿ç”¨ Dask è¿›è¡Œåˆ†å¸ƒå¼è®¡ç®—

**é€‚ç”¨åœºæ™¯**ï¼šå¤„ç†è¶…å¤§è§„æ¨¡ç½‘æ ¼æ•°æ®ï¼ˆGB çº§åˆ«ï¼‰ã€‚

```python
import dask.array as da

# å°†æ•°æ®è½¬ä¸º Dask æ•°ç»„ï¼ˆå»¶è¿Ÿè®¡ç®—ï¼‰
data_dask = da.from_array(large_netcdf_data, chunks=(1000, 1000))

# åˆ†å—è®¡ç®—æç«¯äº‹ä»¶
extreme_mask = data_dask.map_blocks(
    lambda block: detect_extreme_events_hist(block, severity=0.01),
    dtype=bool
)

# è§¦å‘è®¡ç®—
result = extreme_mask.compute()
```

#### ç­–ç•¥ 3: Cython é‡å†™å…³é”®å‡½æ•°

**é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦æè‡´æ€§èƒ½ä¸” Numba ä¸é€‚ç”¨çš„æƒ…å†µã€‚

```cython
# extreme_detection_cython.pyx
import numpy as np
cimport numpy as np

cpdef list identify_extremes_cython(np.ndarray[np.int32_t, ndim=1] mask, int min_duration):
    cdef int i = 0
    cdef int n = len(mask)
    cdef int start, end
    cdef list events = []

    while i < n:
        if mask[i]:
            start = i
            while i < n and mask[i]:
                i += 1
            end = i
            if end - start >= min_duration:
                events.append((start, end))
        else:
            i += 1

    return events
```

---

## 6. æ€§èƒ½åŸºå‡†æµ‹è¯•

### 6.1 æµ‹è¯•ç¯å¢ƒ

```python
import platform
import psutil

print(f"OS: {platform.system()} {platform.release()}")
print(f"CPU: {platform.processor()}")
print(f"Cores: {psutil.cpu_count(logical=False)} physical, {psutil.cpu_count(logical=True)} logical")
print(f"RAM: {psutil.virtual_memory().total / 1e9:.1f} GB")
print(f"Python: {platform.python_version()}")
print(f"NumPy: {np.__version__}")
```

### 6.2 åŸºå‡†æµ‹è¯•è„šæœ¬

```python
import time
import numpy as np
from src.extreme_detection import (
    detect_extreme_events_hist,
    detect_extreme_events_clim,
    optimal_path_threshold
)

def benchmark_detection_methods(n_years=40):
    """
    åŸºå‡†æµ‹è¯•ï¼šå¯¹æ¯”ä¸åŒæ£€æµ‹æ–¹æ³•çš„æ€§èƒ½
    """
    n_days = n_years * 365
    np.random.seed(42)

    # ç”Ÿæˆåˆæˆæ•°æ®
    doy = np.tile(np.arange(365), n_years)
    et0 = 4.0 + 2.5 * np.sin(2 * np.pi * (doy - 80) / 365) + np.random.normal(0, 0.6, n_days)

    results = {}

    # ERT_hist
    start = time.time()
    mask_hist, _ = detect_extreme_events_hist(et0, severity=0.01)
    results['ERT_hist'] = time.time() - start

    # ERT_clim
    start = time.time()
    mask_clim, _ = detect_extreme_events_clim(et0, severity=0.05, min_duration=3)
    results['ERT_clim'] = time.time() - start

    # OPT
    start = time.time()
    try:
        mask_opt, _ = optimal_path_threshold(et0, target_severity=0.01, max_iterations=20)
        results['OPT'] = time.time() - start
    except Exception as e:
        results['OPT'] = f"Error: {e}"

    # æ‰“å°ç»“æœ
    print(f"\n{'='*60}")
    print(f"Benchmark Results ({n_years} years, {n_days} days)")
    print(f"{'='*60}")
    for method, elapsed in results.items():
        if isinstance(elapsed, float):
            print(f"{method:15s}: {elapsed:.4f}s")
        else:
            print(f"{method:15s}: {elapsed}")
    print(f"{'='*60}\n")

    return results

# è¿è¡ŒåŸºå‡†æµ‹è¯•
benchmark_detection_methods(n_years=40)
```

### 6.3 å†…å­˜åˆ†æ

```python
from memory_profiler import profile

@profile
def memory_intensive_function():
    """
    ä½¿ç”¨ memory_profiler åˆ†æå†…å­˜ä½¿ç”¨

    è¿è¡Œæ–¹å¼ï¼š
    python -m memory_profiler script.py
    """
    data = np.random.rand(10000, 1000)  # 10,000 ä¸ªç«™ç‚¹ï¼Œ1000 å¤©
    correlation_matrix = np.corrcoef(data)  # éœ€è¦ ~800 MB
    return correlation_matrix
```

---

## 7. å†…å­˜ä¼˜åŒ–ç­–ç•¥

### 7.1 åˆ†å—å¤„ç†å¤§æ•°æ®

```python
def process_large_dataset_chunked(filepath, chunk_size=365*5):
    """
    åˆ†å—å¤„ç†å¤§å‹ NetCDF æ–‡ä»¶

    é¿å…ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜
    """
    import xarray as xr

    ds = xr.open_dataset(filepath, chunks={'time': chunk_size})

    results = []
    for chunk in ds.time.groupby('time.year'):
        year, data_chunk = chunk
        # å¤„ç†å•ä¸ªå¹´ä»½çš„æ•°æ®
        et0_chunk = calculate_et0(...)
        results.append(et0_chunk)

        # æ˜¾å¼é‡Šæ”¾å†…å­˜
        del data_chunk
        import gc
        gc.collect()

    return results
```

### 7.2 ä½¿ç”¨å†…å­˜æ˜ å°„æ–‡ä»¶

```python
# å°†ç»“æœä¿å­˜ä¸ºå†…å­˜æ˜ å°„æ•°ç»„ï¼ˆä¸å ç”¨ RAMï¼‰
result_mmap = np.memmap('temp_results.npy', dtype='float32',
                        mode='w+', shape=(n_time, n_lat, n_lon))

# åˆ†å—å†™å…¥
for i in range(n_chunks):
    result_mmap[chunk_start:chunk_end, :, :] = process_chunk(i)

result_mmap.flush()  # å†™å…¥ç£ç›˜
```

---

## 8. å¹¶è¡Œè®¡ç®—æ–¹æ¡ˆ

### 8.1 å¤šçº¿ç¨‹ï¼ˆé€‚ç”¨äº I/O å¯†é›†å‹ï¼‰

```python
from concurrent.futures import ThreadPoolExecutor

def process_single_location(lat, lon, data):
    """å¤„ç†å•ä¸ªä½ç½®çš„æ—¶é—´åºåˆ—"""
    series = extract_series(data, lat, lon)
    mask, _ = detect_extreme_events_clim(series)
    return np.sum(mask)

# å¹¶è¡Œå¤„ç†å¤šä¸ªç«™ç‚¹
locations = [(lat, lon) for lat in lats for lon in lons]

with ThreadPoolExecutor(max_workers=8) as executor:
    results = list(executor.map(
        lambda loc: process_single_location(*loc, data),
        locations
    ))
```

### 8.2 å¤šè¿›ç¨‹ï¼ˆé€‚ç”¨äº CPU å¯†é›†å‹ï¼‰

```python
from multiprocessing import Pool

def worker_function(chunk):
    """å·¥ä½œè¿›ç¨‹ï¼šå¤„ç†ä¸€ä¸ªæ•°æ®å—"""
    return detect_extreme_events_clim(chunk)

if __name__ == '__main__':
    # å°†æ•°æ®åˆ†å‰²ä¸ºå¤šå—
    chunks = np.array_split(large_data, num_cores)

    with Pool(num_cores) as pool:
        results = pool.map(worker_function, chunks)

    # åˆå¹¶ç»“æœ
    final_result = np.concatenate(results)
```

---

## 9. æ€»ç»“ä¸å®æ–½è·¯çº¿å›¾

### 9.1 å¿«é€Ÿä¼˜åŒ–æ¸…å•ï¼ˆæœ€å¤§æŠ•å…¥äº§å‡ºæ¯”ï¼‰

| æ­¥éª¤ | æ“ä½œ | é¢„æœŸæ•ˆæœ | å·¥ä½œé‡ |
|------|------|---------|--------|
| 1 | ä¸º `identify_climatological_extremes` æ·»åŠ  `@jit` è£…é¥°å™¨ | 10-15x | 5 åˆ†é’Ÿ |
| 2 | é‡å†™ `calculate_spatial_correlation` ä½¿ç”¨çŸ©é˜µè¿ç®— | 8-10x | 1 å°æ—¶ |
| 3 | ä¸º `calculate_contributions` æ·»åŠ ç»“æœç¼“å­˜ | 2-3x | 30 åˆ†é’Ÿ |
| 4 | ä¸ºå¤§æ•°æ®é›†æ·»åŠ åˆ†å—å¤„ç†é€»è¾‘ | é¿å… OOM | 1 å°æ—¶ |

### 9.2 é•¿æœŸä¼˜åŒ–è®¡åˆ’

1. **Phase 1ï¼ˆ1-2å‘¨ï¼‰**ï¼š
   - å®æ–½ä¸Šè¿°å¿«é€Ÿä¼˜åŒ–
   - æ·»åŠ æ€§èƒ½åŸºå‡†æµ‹è¯•
   - æ›´æ–°æ–‡æ¡£

2. **Phase 2ï¼ˆ1ä¸ªæœˆï¼‰**ï¼š
   - ä½¿ç”¨ Cython é‡å†™æ ¸å¿ƒå¾ªç¯
   - å®ç°åˆ†å¸ƒå¼è®¡ç®—ï¼ˆDaskï¼‰
   - GPU åŠ é€Ÿæ¢ç´¢ï¼ˆCuPyï¼‰

3. **Phase 3ï¼ˆæŒç»­ï¼‰**ï¼š
   - å»ºç«‹æŒç»­é›†æˆçš„æ€§èƒ½ç›‘æ§
   - å®šæœŸè¿›è¡Œ profiling
   - æ”¶é›†ç”¨æˆ·åé¦ˆä¼˜åŒ–çƒ­ç‚¹

---

## 10. å‚è€ƒèµ„æº

### æ€§èƒ½åˆ†æå·¥å…·

1. **cProfile**: Python æ ‡å‡†åº“ï¼Œå‡½æ•°çº§æ€§èƒ½åˆ†æ
2. **line_profiler**: è¡Œçº§æ€§èƒ½åˆ†æ
3. **memory_profiler**: å†…å­˜ä½¿ç”¨åˆ†æ
4. **py-spy**: æ— ä¾µå…¥æ€§çš„ profiler

### ä¼˜åŒ–åº“

1. **Numba**: JIT ç¼–è¯‘å™¨ (https://numba.pydata.org/)
2. **Cython**: Python åˆ° C çš„è½¬è¯‘å™¨ (https://cython.org/)
3. **Dask**: å¹¶è¡Œè®¡ç®—åº“ (https://dask.org/)
4. **CuPy**: GPU åŠ é€Ÿçš„ NumPy (https://cupy.dev/)

### æ¨èé˜…è¯»

1. *High Performance Python* by Micha Gorelick & Ian Ozsvald
2. NumPy documentation on broadcasting and vectorization
3. SciPy optimization tutorials
