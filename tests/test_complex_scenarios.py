"""
å¤æ‚åœºæ™¯é›†æˆæµ‹è¯• (Complex Scenario Integration Tests)

æœ¬æ¨¡å—å®ç°äº†çœŸå®ç§‘ç ”åœºæ™¯çš„ç«¯åˆ°ç«¯æµ‹è¯•ï¼ŒåŒ…æ‹¬ï¼š
1. å®Œæ•´çš„ I/O æµç¨‹ï¼ˆNetCDF è¯»å†™ï¼‰
2. å¤æ‚çš„æ—¶é—´åºåˆ—å¤„ç†
3. ç©ºé—´æ•°æ®å¤„ç†
4. å¤šæ–¹æ³•å¯¹æ¯”éªŒè¯
5. æ€§èƒ½å‹åŠ›æµ‹è¯•

ä½œè€…: Extreme-ET Team
æ—¥æœŸ: 2025
"""

import pytest
import numpy as np
import pandas as pd
from pathlib import Path
import tempfile


# ============================================================================
# æµ‹è¯•å¤¹å…· (Test Fixtures)
# ============================================================================

@pytest.fixture
def mock_netcdf_file(tmp_path):
    """
    åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„ NetCDF æ–‡ä»¶

    åŒ…å«ï¼š
    - 2å¹´çš„æ—¥æ•°æ®ï¼ˆ730å¤©ï¼‰
    - 5x5 çš„ç©ºé—´ç½‘æ ¼
    - ET0ã€æ¸©åº¦ã€è¾å°„ç­‰å˜é‡
    - é¢„åŸ‹çš„æç«¯äº‹ä»¶ï¼ˆåœ¨ç‰¹å®šä½ç½®å’Œæ—¶é—´ï¼‰
    """
    try:
        import xarray as xr
    except ImportError:
        pytest.skip("xarray is required for this test")

    # åˆ›å»ºç©ºé—´åæ ‡
    lat = np.linspace(30, 40, 5)
    lon = np.linspace(-100, -90, 5)
    time = pd.date_range("2000-01-01", periods=365*2, freq="D")

    # åˆ›å»ºåŸºç¡€æ•°æ®ï¼ˆéšæœº + å­£èŠ‚æ€§ï¼‰
    n_time = len(time)
    n_lat = len(lat)
    n_lon = len(lon)

    # ET0: åŸºç¡€å€¼ 3-7 mm/dayï¼Œæœ‰å­£èŠ‚å˜åŒ–
    doy = time.dayofyear.values
    seasonal_cycle = 2 + 2.5 * np.sin(2 * np.pi * (doy - 80) / 365)  # å¤é«˜å†¬ä½
    et0_data = np.zeros((n_time, n_lat, n_lon))

    for i in range(n_lat):
        for j in range(n_lon):
            et0_data[:, i, j] = seasonal_cycle + np.random.normal(0, 0.3, n_time)

    # ğŸ”¥ é¢„åŸ‹æç«¯äº‹ä»¶
    # äº‹ä»¶1: åœ¨ç½‘æ ¼ç‚¹ (2, 2) çš„ç¬¬100-105å¤©
    et0_data[100:105, 2, 2] += 5.0

    # äº‹ä»¶2: åœ¨ç½‘æ ¼ç‚¹ (3, 3) çš„ç¬¬200-207å¤©ï¼ˆæŒç»­æ—¶é—´æ›´é•¿ï¼‰
    et0_data[200:207, 3, 3] += 4.5

    # æ¸©åº¦æ•°æ®
    temp_mean = 15 + 10 * np.sin(2 * np.pi * (doy - 80) / 365)
    temp_data = np.tile(temp_mean[:, np.newaxis, np.newaxis], (1, n_lat, n_lon))
    temp_data += np.random.normal(0, 1.5, (n_time, n_lat, n_lon))

    # è¾å°„æ•°æ®
    rad_data = 15 + 10 * np.sin(2 * np.pi * (doy - 80) / 365)
    rad_data = np.tile(rad_data[:, np.newaxis, np.newaxis], (1, n_lat, n_lon))
    rad_data += np.random.normal(0, 1.0, (n_time, n_lat, n_lon))
    rad_data = np.maximum(rad_data, 0)  # ç¡®ä¿éè´Ÿ

    # åˆ›å»º xarray Dataset
    ds = xr.Dataset(
        {
            "et0": (["time", "lat", "lon"], et0_data),
            "temperature": (["time", "lat", "lon"], temp_data),
            "radiation": (["time", "lat", "lon"], rad_data),
        },
        coords={
            "time": time,
            "lat": lat,
            "lon": lon,
        },
        attrs={
            "title": "Mock ET Data for Testing",
            "institution": "Extreme-ET Test Suite",
            "source": "Synthetic data with embedded extreme events"
        }
    )

    # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
    file_path = tmp_path / "mock_et_data.nc"
    ds.to_netcdf(file_path)

    # è¿”å›æ–‡ä»¶è·¯å¾„å’Œæç«¯äº‹ä»¶çš„ä½ç½®ä¿¡æ¯
    extreme_events = {
        'event1': {
            'lat_idx': 2,
            'lon_idx': 2,
            'time_range': (100, 105),
            'lat': lat[2],
            'lon': lon[2]
        },
        'event2': {
            'lat_idx': 3,
            'lon_idx': 3,
            'time_range': (200, 207),
            'lat': lat[3],
            'lon': lon[3]
        }
    }

    return file_path, extreme_events


@pytest.fixture
def sample_station_data():
    """
    åˆ›å»ºæ¨¡æ‹Ÿçš„ç«™ç‚¹è§‚æµ‹æ•°æ®ï¼ˆç”¨äºéªŒè¯ï¼‰

    è¿”å›ä¸€ä¸ª pandas DataFrameï¼ŒåŒ…å«ï¼š
    - 3å¹´çš„æ—¥æ•°æ®
    - å®Œæ•´çš„æ°”è±¡å˜é‡
    - ä¸€äº›ç¼ºå¤±å€¼ï¼ˆæµ‹è¯•æ’å€¼ï¼‰
    - ä¸€äº›å¼‚å¸¸å€¼ï¼ˆæµ‹è¯•è´¨é‡æ§åˆ¶ï¼‰
    """
    dates = pd.date_range("2000-01-01", periods=365*3, freq="D")
    n = len(dates)

    # åŸºç¡€å­£èŠ‚æ€§æ¨¡å¼
    doy = dates.dayofyear.values
    temp_mean = 15 + 10 * np.sin(2 * np.pi * (doy - 80) / 365) + np.random.normal(0, 2, n)
    temp_max = temp_mean + 5 + np.random.normal(0, 1, n)
    temp_min = temp_mean - 5 + np.random.normal(0, 1, n)
    radiation = np.maximum(10 + 8 * np.sin(2 * np.pi * (doy - 80) / 365) + np.random.normal(0, 1.5, n), 0)
    wind = np.abs(2.0 + np.random.normal(0, 0.5, n))

    # æ°´æ±½å‹ï¼ˆä»æ¸©åº¦ä¼°ç®—ï¼‰
    es = 0.6108 * np.exp(17.27 * temp_mean / (temp_mean + 237.3))
    rh = 60 + 20 * np.random.rand(n)  # ç›¸å¯¹æ¹¿åº¦ 40-80%
    ea = es * rh / 100

    df = pd.DataFrame({
        'T_mean': temp_mean,
        'T_max': temp_max,
        'T_min': temp_min,
        'Rs': radiation,
        'u2': wind,
        'ea': ea
    }, index=dates)

    # ğŸ”§ åˆ¶é€ ä¸€äº›ç¼ºå¤±å€¼ï¼ˆæ¨¡æ‹ŸçœŸå®æƒ…å†µï¼‰
    missing_indices = np.random.choice(n, size=int(n * 0.02), replace=False)  # 2% ç¼ºå¤±
    df.loc[df.index[missing_indices], 'Rs'] = np.nan

    # åˆ¶é€ ä¸€æ®µè¿ç»­ç¼ºå¤±ï¼ˆæµ‹è¯•æ’å€¼æé™ï¼‰
    df.loc[df.index[500:506], 'u2'] = np.nan  # è¿ç»­6å¤©ç¼ºå¤±

    # ğŸš¨ åˆ¶é€ ä¸€äº›å¼‚å¸¸å€¼ï¼ˆæµ‹è¯•è´¨é‡æ§åˆ¶ï¼‰
    df.loc[df.index[100], 'T_max'] = 999.9  # æ˜æ˜¾å¼‚å¸¸
    df.loc[df.index[200], 'Rs'] = -5.0  # ç‰©ç†ä¸Šä¸å¯èƒ½çš„è´Ÿå€¼

    return df


# ============================================================================
# æµ‹è¯• 1: ç«¯åˆ°ç«¯ I/O å’Œæç«¯æ£€æµ‹æµç¨‹
# ============================================================================

def test_end_to_end_io_and_detection(mock_netcdf_file):
    """
    æµ‹è¯•å®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹ï¼š
    1. ä» NetCDF è¯»å–æ•°æ®
    2. æå–ç‰¹å®šç«™ç‚¹æ—¶é—´åºåˆ—
    3. ä½¿ç”¨æ°”å€™å­¦æ–¹æ³•æ£€æµ‹æç«¯äº‹ä»¶
    4. éªŒè¯é¢„åŸ‹çš„æç«¯äº‹ä»¶æ˜¯å¦è¢«æ­£ç¡®æ£€æµ‹
    """
    try:
        import xarray as xr
    except ImportError:
        pytest.skip("xarray is required for this test")

    from src.extreme_detection import detect_extreme_events_clim

    file_path, extreme_events = mock_netcdf_file

    # 1. è¯»å– NetCDF æ•°æ®
    ds = xr.open_dataset(file_path)
    assert 'et0' in ds.data_vars, "ET0 variable not found in dataset"

    # 2. æå–ç¬¬ä¸€ä¸ªé¢„åŸ‹äº‹ä»¶çš„ä½ç½®
    event1 = extreme_events['event1']
    lat_idx = event1['lat_idx']
    lon_idx = event1['lon_idx']

    et0_series = ds['et0'][:, lat_idx, lon_idx].values

    # 3. è¿è¡Œæç«¯æ£€æµ‹ï¼ˆERT_climï¼‰
    # ä½¿ç”¨è¾ƒå®½æ¾çš„é˜ˆå€¼ï¼Œç¡®ä¿èƒ½æ£€æµ‹åˆ°é¢„åŸ‹äº‹ä»¶
    extreme_mask, thresholds, details = detect_extreme_events_clim(
        et0_series,
        severity=0.10,  # å‰ 10%
        min_duration=3,
        return_details=True
    )

    # 4. éªŒè¯æ£€æµ‹ç»“æœ
    event1_range = range(event1['time_range'][0], event1['time_range'][1])

    # æ£€æŸ¥é¢„åŸ‹äº‹ä»¶æœŸé—´è‡³å°‘æœ‰éƒ¨åˆ†è¢«æ ‡è®°ä¸ºæç«¯
    detected_in_event = np.sum(extreme_mask[event1_range])
    assert detected_in_event >= 3, \
        f"Failed to detect event1: only {detected_in_event}/5 days detected"

    # 5. æ£€æŸ¥è¿”å›çš„ç»Ÿè®¡ä¿¡æ¯
    assert 'n_events' in details, "Details should contain number of events"
    assert details['n_events'] >= 1, "At least one event should be detected"
    assert details['total_extreme_days'] >= 5, "Should detect at least 5 extreme days"

    print(f"âœ“ Test passed: Detected {details['n_events']} events, "
          f"{details['total_extreme_days']} extreme days")


# ============================================================================
# æµ‹è¯• 2: æ•°æ®æ¸…æ´—ä¸è´¨é‡æ§åˆ¶æµç¨‹
# ============================================================================

def test_data_quality_control_pipeline(sample_station_data):
    """
    æµ‹è¯•æ•°æ®è´¨é‡æ§åˆ¶æµç¨‹ï¼š
    1. æ£€æµ‹å¼‚å¸¸å€¼
    2. å¤„ç†ç¼ºå¤±å€¼
    3. éªŒè¯æ¸©åº¦é€»è¾‘ä¸€è‡´æ€§
    4. ç¡®ä¿å¤„ç†åçš„æ•°æ®å¯ç”¨äº ET0 è®¡ç®—
    """
    from src.penman_monteith import calculate_et0

    df_original = sample_station_data.copy()

    # ========== æ­¥éª¤ 1: ç‰©ç†èŒƒå›´æ£€æŸ¥ ==========
    def check_and_clean_physical_bounds(df):
        bounds = {
            'T_mean': (-60, 60),
            'T_max': (-50, 70),
            'T_min': (-70, 50),
            'Rs': (0, 40),
            'u2': (0, 30),
            'ea': (0, 7)
        }

        df_clean = df.copy()
        for var, (lower, upper) in bounds.items():
            if var in df_clean.columns:
                invalid = (df_clean[var] < lower) | (df_clean[var] > upper)
                n_invalid = np.sum(invalid)
                if n_invalid > 0:
                    print(f"  Cleaning {var}: {n_invalid} out-of-bounds values")
                    df_clean.loc[invalid, var] = np.nan

        return df_clean

    df_clean = check_and_clean_physical_bounds(df_original)

    # éªŒè¯ï¼šå¼‚å¸¸çš„ T_max=999.9 åº”è¯¥è¢«æ¸…é™¤
    assert not np.any(df_clean['T_max'] > 100), "Outlier not removed"

    # ========== æ­¥éª¤ 2: æ¸©åº¦é€»è¾‘ä¸€è‡´æ€§ ==========
    def fix_temperature_consistency(df):
        df_fixed = df.copy()
        # å¦‚æœ T_min > T_maxï¼Œäº¤æ¢å®ƒä»¬
        swap_mask = df_fixed['T_min'] > df_fixed['T_max']
        if np.any(swap_mask):
            print(f"  Fixing {np.sum(swap_mask)} inconsistent temperature records")
            df_fixed.loc[swap_mask, ['T_min', 'T_max']] = \
                df_fixed.loc[swap_mask, ['T_max', 'T_min']].values

        # é‡æ–°è®¡ç®— T_meanï¼ˆå¦‚æœä¸ä¸€è‡´ï¼‰
        expected_mean = (df_fixed['T_max'] + df_fixed['T_min']) / 2
        inconsistent = np.abs(df_fixed['T_mean'] - expected_mean) > 5
        if np.any(inconsistent):
            print(f"  Recalculating T_mean for {np.sum(inconsistent)} records")
            df_fixed.loc[inconsistent, 'T_mean'] = expected_mean[inconsistent]

        return df_fixed

    df_clean = fix_temperature_consistency(df_clean)

    # ========== æ­¥éª¤ 3: ç¼ºå¤±å€¼æ’å€¼ ==========
    def interpolate_missing(df, max_gap=7):
        df_filled = df.copy()
        for col in df_filled.columns:
            df_filled[col] = df_filled[col].interpolate(
                method='linear',
                limit=max_gap,
                limit_direction='both'
            )
        return df_filled

    df_clean = interpolate_missing(df_clean, max_gap=7)

    # éªŒè¯ï¼šçŸ­ç¼ºå¤±åº”è¯¥è¢«å¡«è¡¥
    assert df_clean['Rs'].isnull().sum() < df_original['Rs'].isnull().sum(), \
        "Interpolation did not reduce missing values"

    # éªŒè¯ï¼šé•¿ç¼ºå¤±ï¼ˆ6å¤©ï¼‰åº”è¯¥è¢«å¡«è¡¥ï¼ˆå› ä¸º < max_gap=7ï¼‰
    assert df_clean['u2'].isnull().sum() == 0, \
        "6-day gap should be filled with max_gap=7"

    # ========== æ­¥éª¤ 4: è®¡ç®— ET0ï¼ˆéªŒè¯æ•°æ®å¯ç”¨æ€§ï¼‰==========
    et0 = calculate_et0(
        T_mean=df_clean['T_mean'].values,
        T_max=df_clean['T_max'].values,
        T_min=df_clean['T_min'].values,
        Rs=df_clean['Rs'].values,
        u2=df_clean['u2'].values,
        ea=df_clean['ea'].values,
        z=50.0,
        latitude=40.0
    )

    # éªŒè¯ ET0 çš„åˆç†æ€§
    assert np.all(et0 >= 0), "ET0 should be non-negative"
    assert np.all(et0 < 20), "ET0 should be less than 20 mm/day"
    assert 2 < np.mean(et0) < 8, f"Mean ET0 ({np.mean(et0):.2f}) is unrealistic"

    print(f"âœ“ Test passed: Data cleaned and ET0 calculated successfully")
    print(f"  Mean ET0: {np.mean(et0):.2f} mm/day")
    print(f"  Remaining missing values: {np.sum(np.isnan(et0))}")


# ============================================================================
# æµ‹è¯• 3: å¤šæ–¹æ³•å¯¹æ¯”æµ‹è¯•
# ============================================================================

def test_multiple_detection_methods_comparison():
    """
    å¯¹æ¯”ä¸‰ç§æ£€æµ‹æ–¹æ³•ï¼ˆERT_hist, ERT_clim, OPTï¼‰ï¼š
    1. åœ¨ç›¸åŒæ•°æ®ä¸Šè¿è¡Œä¸‰ç§æ–¹æ³•
    2. éªŒè¯æ£€æµ‹ç»“æœçš„åˆç†æ€§
    3. æ¯”è¾ƒæ–¹æ³•é—´çš„å·®å¼‚
    """
    from src.extreme_detection import (
        detect_extreme_events_hist,
        detect_extreme_events_clim,
        optimal_path_threshold
    )

    # åˆ›å»ºåˆæˆæ•°æ®ï¼ˆ10å¹´ï¼‰
    np.random.seed(42)
    n_years = 10
    n_days = n_years * 365

    # åŸºç¡€å­£èŠ‚æ€§ + éšæœºå™ªå£°
    doy = np.tile(np.arange(365), n_years)
    et0 = 3.5 + 2.5 * np.sin(2 * np.pi * (doy - 80) / 365) + np.random.normal(0, 0.5, n_days)

    # åˆ¶é€ å‡ ä¸ªæç«¯äº‹ä»¶
    extreme_indices = [100, 101, 102, 500, 501, 502, 503, 1000, 1001, 1002]
    et0[extreme_indices] += 4.0

    # ========== è¿è¡Œä¸‰ç§æ–¹æ³• ==========
    target_severity = 0.01  # å‰ 1% (~3.65 å¤©/å¹´)

    # æ–¹æ³•1: ERT_hist
    mask_hist, threshold_hist = detect_extreme_events_hist(
        et0, severity=target_severity
    )

    # æ–¹æ³•2: ERT_clim
    mask_clim, thresholds_clim = detect_extreme_events_clim(
        et0, severity=0.05, min_duration=3
    )

    # æ–¹æ³•3: OPTï¼ˆä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬ï¼Œå¦‚æœå®ç°äº†çš„è¯ï¼‰
    try:
        mask_opt, thresholds_opt = optimal_path_threshold(
            et0, target_severity=target_severity
        )
    except (NameError, AttributeError):
        pytest.skip("OPT method not implemented")
        mask_opt = None

    # ========== éªŒè¯æ£€æµ‹ç»“æœ ==========
    n_detected = {
        'ERT_hist': np.sum(mask_hist),
        'ERT_clim': np.sum(mask_clim),
        'OPT': np.sum(mask_opt) if mask_opt is not None else None
    }

    print("\n=== Detection Method Comparison ===")
    for method, count in n_detected.items():
        if count is not None:
            rate = count / n_days * 100
            print(f"{method:12s}: {count:4d} days ({rate:.2f}%)")

    # éªŒè¯1: ERT_hist åº”è¯¥æ¥è¿‘ç›®æ ‡ä¸¥é‡æ€§
    expected_days = n_days * target_severity
    assert abs(n_detected['ERT_hist'] - expected_days) < expected_days * 0.2, \
        f"ERT_hist detected {n_detected['ERT_hist']} days, expected ~{expected_days}"

    # éªŒè¯2: æ–¹æ³•é—´çš„é‡å åº¦
    overlap_hist_clim = np.sum(mask_hist & mask_clim)
    overlap_rate = overlap_hist_clim / np.sum(mask_hist) if np.sum(mask_hist) > 0 else 0

    print(f"\nOverlap between ERT_hist and ERT_clim: {overlap_rate:.1%}")

    # éªŒè¯3: è‡³å°‘æœ‰ä¸€ä¸ªæ–¹æ³•æ£€æµ‹åˆ°é¢„åŸ‹çš„æç«¯äº‹ä»¶
    detected_embedded = np.sum(mask_hist[extreme_indices]) + np.sum(mask_clim[extreme_indices])
    assert detected_embedded >= 5, \
        f"Methods should detect at least 5 of the 10 embedded extreme days"

    print(f"Embedded extremes detected: {detected_embedded}/10")
    print("âœ“ Test passed: All methods produced reasonable results")


# ============================================================================
# æµ‹è¯• 4: ç©ºé—´åˆ†ææµ‹è¯•
# ============================================================================

def test_spatial_analysis_on_gridded_data(mock_netcdf_file):
    """
    æµ‹è¯•ç©ºé—´åˆ†æåŠŸèƒ½ï¼š
    1. è®¡ç®—åŒºåŸŸå¹³å‡
    2. è¯†åˆ«ç©ºé—´ç›¸å…³çš„æç«¯äº‹ä»¶
    3. ç©ºé—´æ’å€¼
    """
    try:
        import xarray as xr
    except ImportError:
        pytest.skip("xarray is required for this test")

    from src.extreme_detection import detect_extreme_events_hist

    file_path, extreme_events = mock_netcdf_file

    # è¯»å–æ•°æ®
    ds = xr.open_dataset(file_path)
    et0_3d = ds['et0'].values  # (time, lat, lon)

    # ========== æµ‹è¯•1: åŒºåŸŸå¹³å‡ ==========
    regional_mean = np.mean(et0_3d, axis=(1, 2))  # å¯¹ç©ºé—´å¹³å‡

    # éªŒè¯ï¼šåŒºåŸŸå¹³å‡åº”è¯¥å¹³æ»‘åŒ–æç«¯å€¼
    max_grid_value = np.max(et0_3d)
    max_regional_value = np.max(regional_mean)
    assert max_regional_value < max_grid_value, \
        "Regional mean should be smoother than grid points"

    print(f"Max grid ET0: {max_grid_value:.2f} mm/day")
    print(f"Max regional mean ET0: {max_regional_value:.2f} mm/day")

    # ========== æµ‹è¯•2: é€æ ¼ç‚¹æ£€æµ‹ ==========
    n_lat, n_lon = et0_3d.shape[1], et0_3d.shape[2]
    detected_count = np.zeros((n_lat, n_lon))

    for i in range(n_lat):
        for j in range(n_lon):
            series = et0_3d[:, i, j]
            mask, _ = detect_extreme_events_hist(series, severity=0.05)
            detected_count[i, j] = np.sum(mask)

    # éªŒè¯ï¼šé¢„åŸ‹äº‹ä»¶çš„ä½ç½®åº”è¯¥æœ‰æ›´å¤šæ£€æµ‹
    event1 = extreme_events['event1']
    event1_count = detected_count[event1['lat_idx'], event1['lon_idx']]

    mean_count = np.mean(detected_count)
    assert event1_count > mean_count * 1.5, \
        f"Event location should have more detections ({event1_count:.0f} vs mean {mean_count:.0f})"

    print(f"Event location extreme days: {event1_count:.0f}")
    print(f"Mean extreme days: {mean_count:.0f}")

    # ========== æµ‹è¯•3: ç©ºé—´ç›¸å…³æ€§ ==========
    # æ£€æŸ¥ç›¸é‚»æ ¼ç‚¹çš„æç«¯äº‹ä»¶æ˜¯å¦æœ‰ç©ºé—´èšé›†
    event2 = extreme_events['event2']
    i2, j2 = event2['lat_idx'], event2['lon_idx']

    # æ£€æŸ¥å‘¨å›´8ä¸ªæ ¼ç‚¹
    neighbors_count = []
    for di in [-1, 0, 1]:
        for dj in [-1, 0, 1]:
            ni, nj = i2 + di, j2 + dj
            if 0 <= ni < n_lat and 0 <= nj < n_lon and not (di == 0 and dj == 0):
                neighbors_count.append(detected_count[ni, nj])

    # ç›¸é‚»æ ¼ç‚¹çš„å¹³å‡æ£€æµ‹æ•°åº”è¯¥é«˜äºå…¨å±€å¹³å‡ï¼ˆç©ºé—´ç›¸å…³æ€§ï¼‰
    neighbor_mean = np.mean(neighbors_count)
    print(f"Neighbor mean extreme days: {neighbor_mean:.0f}")

    # æ³¨æ„ï¼šè¿™ä¸ªæµ‹è¯•å¯èƒ½ä¸ç¨³å®šï¼Œå› ä¸ºæˆ‘ä»¬åªé¢„åŸ‹äº†å­¤ç«‹äº‹ä»¶
    # åœ¨çœŸå®æ•°æ®ä¸­ï¼Œæç«¯äº‹ä»¶é€šå¸¸æœ‰ç©ºé—´ç›¸å…³æ€§

    print("âœ“ Test passed: Spatial analysis completed")


# ============================================================================
# æµ‹è¯• 5: é©±åŠ¨å› å­è´¡çŒ®åˆ†ææµ‹è¯•
# ============================================================================

def test_driver_contribution_analysis(sample_station_data):
    """
    æµ‹è¯•é©±åŠ¨å› å­è´¡çŒ®ç‡åˆ†æï¼š
    1. è®¡ç®— ET0
    2. æ£€æµ‹æç«¯äº‹ä»¶
    3. åˆ†æå„é©±åŠ¨å› å­çš„è´¡çŒ®
    4. éªŒè¯è´¡çŒ®ç‡æ€»å’Œä¸º 100%
    """
    from src.penman_monteith import calculate_et0
    from src.extreme_detection import detect_extreme_events_hist
    from src.contribution_analysis import calculate_contributions

    df = sample_station_data.copy()

    # æ•°æ®æ¸…æ´—ï¼ˆç®€åŒ–ç‰ˆï¼‰
    df = df.interpolate(method='linear', limit=7)

    # ========== è®¡ç®— ET0 ==========
    et0 = calculate_et0(
        T_mean=df['T_mean'].values,
        T_max=df['T_max'].values,
        T_min=df['T_min'].values,
        Rs=df['Rs'].values,
        u2=df['u2'].values,
        ea=df['ea'].values,
        z=50.0,
        latitude=40.0
    )

    # ========== æ£€æµ‹æç«¯äº‹ä»¶ ==========
    extreme_mask, _ = detect_extreme_events_hist(et0, severity=0.05)
    n_extreme = np.sum(extreme_mask)

    assert n_extreme > 0, "Should detect at least some extreme events"
    print(f"Detected {n_extreme} extreme days for contribution analysis")

    # ========== è®¡ç®—è´¡çŒ®ç‡ ==========
    contributions = calculate_contributions(
        T_mean=df['T_mean'].values,
        T_max=df['T_max'].values,
        T_min=df['T_min'].values,
        Rs=df['Rs'].values,
        u2=df['u2'].values,
        ea=df['ea'].values,
        extreme_mask=extreme_mask,
        z=50.0,
        latitude=40.0
    )

    # ========== éªŒè¯ç»“æœ ==========
    print("\nDriver Contributions:")
    for factor, contrib in contributions.items():
        print(f"  {factor:15s}: {contrib:6.2f}%")

    # éªŒè¯1: æ‰€æœ‰è´¡çŒ®ç‡åº”è¯¥åœ¨åˆç†èŒƒå›´å†…
    for factor, contrib in contributions.items():
        assert 0 <= contrib <= 100, \
            f"{factor} contribution ({contrib:.2f}%) is out of range [0, 100]"

    # éªŒè¯2: è´¡çŒ®ç‡æ€»å’Œåº”è¯¥æ¥è¿‘ 100%ï¼ˆå…è®¸å°è¯¯å·®ï¼‰
    total_contrib = sum(contributions.values())
    assert 99 <= total_contrib <= 101, \
        f"Total contribution ({total_contrib:.2f}%) should be ~100%"

    # éªŒè¯3: è‡³å°‘æœ‰ä¸€ä¸ªä¸»å¯¼å› å­ï¼ˆè´¡çŒ® > 30%ï¼‰
    max_contrib = max(contributions.values())
    assert max_contrib > 30, \
        f"At least one factor should be dominant (>30%), but max is {max_contrib:.2f}%"

    max_factor = max(contributions, key=contributions.get)
    print(f"\nDominant driver: {max_factor} ({contributions[max_factor]:.1f}%)")
    print("âœ“ Test passed: Contribution analysis is valid")


# ============================================================================
# æµ‹è¯• 6: æ€§èƒ½å‹åŠ›æµ‹è¯•
# ============================================================================

def test_performance_on_large_dataset():
    """
    æ€§èƒ½æµ‹è¯•ï¼š
    1. æµ‹è¯•å¤§è§„æ¨¡æ•°æ®é›†ï¼ˆ50å¹´æ—¥æ•°æ®ï¼‰çš„å¤„ç†é€Ÿåº¦
    2. éªŒè¯å†…å­˜ä½¿ç”¨åˆç†
    3. ç¡®ä¿ç®—æ³•åœ¨åˆç†æ—¶é—´å†…å®Œæˆ
    """
    import time
    from src.extreme_detection import detect_extreme_events_hist

    # åˆ›å»º50å¹´çš„æ—¥æ•°æ®
    n_years = 50
    n_days = n_years * 365
    print(f"\nTesting with {n_days} days ({n_years} years) of data...")

    np.random.seed(42)
    doy = np.tile(np.arange(365), n_years)
    et0 = 4.0 + 2.0 * np.sin(2 * np.pi * (doy - 80) / 365) + np.random.normal(0, 0.6, n_days)

    # ========== æµ‹è¯• ERT_histï¼ˆåº”è¯¥å¾ˆå¿«ï¼‰==========
    start_time = time.time()
    mask_hist, threshold = detect_extreme_events_hist(et0, severity=0.01)
    elapsed_hist = time.time() - start_time

    print(f"ERT_hist: {elapsed_hist:.3f} seconds")
    assert elapsed_hist < 1.0, \
        f"ERT_hist is too slow: {elapsed_hist:.3f}s (should be < 1s)"

    # ========== æµ‹è¯• ERT_climï¼ˆç¨æ…¢ï¼Œä½†ä»åº”åˆç†ï¼‰==========
    from src.extreme_detection import detect_extreme_events_clim

    start_time = time.time()
    mask_clim, thresholds = detect_extreme_events_clim(
        et0, severity=0.05, min_duration=3
    )
    elapsed_clim = time.time() - start_time

    print(f"ERT_clim: {elapsed_clim:.3f} seconds")
    assert elapsed_clim < 5.0, \
        f"ERT_clim is too slow: {elapsed_clim:.3f}s (should be < 5s)"

    # ========== å†…å­˜æ£€æŸ¥ ==========
    # ç¡®ä¿è¿”å›çš„æ©ç å¤§å°æ­£ç¡®
    assert mask_hist.shape == (n_days,), "Mask shape mismatch"
    assert mask_clim.shape == (n_days,), "Mask shape mismatch"
    assert thresholds.shape == (365,), "Threshold shape mismatch"

    print("âœ“ Performance test passed")
    print(f"  ERT_hist: {elapsed_hist:.3f}s | ERT_clim: {elapsed_clim:.3f}s")


# ============================================================================
# æµ‹è¯• 7: é”™è¯¯å¤„ç†æµ‹è¯•
# ============================================================================

def test_error_handling():
    """
    æµ‹è¯•å„ç§å¼‚å¸¸è¾“å…¥çš„é”™è¯¯å¤„ç†ï¼š
    1. ç©ºæ•°ç»„
    2. å…¨ä¸º NaN çš„æ•°ç»„
    3. é•¿åº¦ä¸åŒ¹é…
    4. æ— æ•ˆçš„å‚æ•°
    """
    from src.extreme_detection import detect_extreme_events_hist
    from src.penman_monteith import calculate_et0

    # ========== æµ‹è¯•1: ç©ºæ•°ç»„ ==========
    with pytest.raises((ValueError, IndexError)):
        detect_extreme_events_hist(np.array([]), severity=0.01)

    # ========== æµ‹è¯•2: å…¨ NaN ==========
    all_nan = np.full(100, np.nan)
    with pytest.raises((ValueError, RuntimeError)):
        detect_extreme_events_hist(all_nan, severity=0.01)

    # ========== æµ‹è¯•3: é•¿åº¦ä¸åŒ¹é… ==========
    T_mean = np.random.rand(100)
    T_max = np.random.rand(100)
    T_min = np.random.rand(100)
    Rs = np.random.rand(100)
    u2 = np.random.rand(100)
    ea = np.random.rand(50)  # é•¿åº¦ä¸åŒ¹é…ï¼

    with pytest.raises((ValueError, IndexError)):
        calculate_et0(T_mean, T_max, T_min, Rs, u2, ea)

    # ========== æµ‹è¯•4: æ— æ•ˆçš„ä¸¥é‡æ€§å‚æ•° ==========
    data = np.random.rand(100)

    with pytest.raises(ValueError):
        detect_extreme_events_hist(data, severity=-0.01)  # è´Ÿæ•°

    with pytest.raises(ValueError):
        detect_extreme_events_hist(data, severity=1.5)  # > 1

    print("âœ“ Error handling test passed")


# ============================================================================
# æµ‹è¯• 8: å›å½’æµ‹è¯•ï¼ˆç¡®ä¿ç»“æœç¨³å®šæ€§ï¼‰
# ============================================================================

def test_regression_stability():
    """
    å›å½’æµ‹è¯•ï¼š
    ç¡®ä¿åœ¨å›ºå®šçš„éšæœºç§å­ä¸‹ï¼Œç»“æœå®Œå…¨å¯é‡å¤
    """
    from src.extreme_detection import detect_extreme_events_hist
    from src.penman_monteith import calculate_et0

    # å›ºå®šéšæœºç§å­
    np.random.seed(12345)

    # ç”Ÿæˆæ•°æ®
    n = 365 * 5
    T_mean = 15 + 10 * np.sin(2 * np.pi * np.arange(n) / 365) + np.random.normal(0, 2, n)
    T_max = T_mean + 5
    T_min = T_mean - 5
    Rs = 15 + 8 * np.sin(2 * np.pi * np.arange(n) / 365) + np.random.normal(0, 1, n)
    u2 = np.abs(2.0 + np.random.normal(0, 0.5, n))
    ea = np.abs(1.5 + np.random.normal(0, 0.3, n))

    # è®¡ç®— ET0
    et0 = calculate_et0(T_mean, T_max, T_min, Rs, u2, ea)

    # æ£€æµ‹æç«¯äº‹ä»¶
    mask, threshold = detect_extreme_events_hist(et0, severity=0.01)

    # æœŸæœ›ç»“æœï¼ˆé€šè¿‡é¦–æ¬¡è¿è¡Œè·å¾—ï¼Œå›ºå®šä¸º"é»„é‡‘æ ‡å‡†"ï¼‰
    expected_n_extreme = int(n * 0.01)  # åº”è¯¥æ¥è¿‘è¿™ä¸ªå€¼
    expected_threshold_range = (6.5, 9.5)  # é˜ˆå€¼åº”è¯¥åœ¨è¿™ä¸ªèŒƒå›´å†…

    # éªŒè¯
    actual_n_extreme = np.sum(mask)
    assert abs(actual_n_extreme - expected_n_extreme) <= expected_n_extreme * 0.3, \
        f"Detected {actual_n_extreme} extremes, expected ~{expected_n_extreme}"

    assert expected_threshold_range[0] <= threshold <= expected_threshold_range[1], \
        f"Threshold {threshold:.2f} is outside expected range {expected_threshold_range}"

    print(f"âœ“ Regression test passed")
    print(f"  Detected: {actual_n_extreme} extremes")
    print(f"  Threshold: {threshold:.2f} mm/day")


# ============================================================================
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
# ============================================================================

if __name__ == "__main__":
    """
    ç›´æ¥è¿è¡Œæ­¤è„šæœ¬è¿›è¡Œå¿«é€Ÿæµ‹è¯•ï¼ˆä¸ä½¿ç”¨ pytestï¼‰
    """
    print("=" * 70)
    print("Running Complex Scenario Integration Tests")
    print("=" * 70)

    import tempfile

    with tempfile.TemporaryDirectory() as tmp_dir:
        tmp_path = Path(tmp_dir)

        # åˆ›å»ºå¤¹å…·
        print("\n[1/8] Setting up mock data...")
        mock_nc, events = pytest.fixture()(mock_netcdf_file)(tmp_path)

        print("\n[2/8] Test: End-to-end I/O and detection...")
        try:
            test_end_to_end_io_and_detection((mock_nc, events))
        except Exception as e:
            print(f"  âœ— FAILED: {e}")

        print("\n[3/8] Test: Data quality control...")
        sample_data = pytest.fixture()(sample_station_data)()
        try:
            test_data_quality_control_pipeline(sample_data)
        except Exception as e:
            print(f"  âœ— FAILED: {e}")

        print("\n[4/8] Test: Multiple detection methods...")
        try:
            test_multiple_detection_methods_comparison()
        except Exception as e:
            print(f"  âœ— FAILED: {e}")

        print("\n[5/8] Test: Spatial analysis...")
        try:
            test_spatial_analysis_on_gridded_data((mock_nc, events))
        except Exception as e:
            print(f"  âœ— FAILED: {e}")

        print("\n[6/8] Test: Driver contribution...")
        try:
            test_driver_contribution_analysis(sample_data)
        except Exception as e:
            print(f"  âœ— FAILED: {e}")

        print("\n[7/8] Test: Performance...")
        try:
            test_performance_on_large_dataset()
        except Exception as e:
            print(f"  âœ— FAILED: {e}")

        print("\n[8/9] Test: Error handling...")
        try:
            test_error_handling()
        except Exception as e:
            print(f"  âœ— FAILED: {e}")

        print("\n[9/9] Test: Regression stability...")
        try:
            test_regression_stability()
        except Exception as e:
            print(f"  âœ— FAILED: {e}")
    print("\n" + "=" * 70)
    print("All tests completed!")
    print("=" * 70)
